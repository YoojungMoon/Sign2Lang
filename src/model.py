#!/usr/bin/env python3
"""
SU:DA - ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸
BiLSTM ê¸°ë°˜ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional
import math

from utils import (
    setup_logger, KEYPOINT_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT,
    TOTAL_LANDMARKS, count_parameters
)

class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”© (ì„ íƒì  ì‚¬ìš©)"""
    
    def __init__(self, d_model: int, max_len: int = 200):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [seq_len, batch_size, d_model]
        Returns:
            ìœ„ì¹˜ ì¸ì½”ë”©ì´ ì¶”ê°€ëœ í…ì„œ
        """
        return x + self.pe[:x.size(0), :]

class AttentionLayer(nn.Module):
    """ì–´í…ì…˜ ë ˆì´ì–´"""
    
    def __init__(self, hidden_dim: int, dropout: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=False
        )
        self.norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: [seq_len, batch_size, hidden_dim]
            mask: [batch_size, seq_len] íŒ¨ë”© ë§ˆìŠ¤í¬
        Returns:
            ì–´í…ì…˜ ì ìš©ëœ í…ì„œ
        """
        # Self-attention
        if mask is not None:
            # maskë¥¼ attentionìš©ìœ¼ë¡œ ë³€í™˜ [batch_size, seq_len] -> [batch_size, seq_len]
            # íŒ¨ë”©ëœ ë¶€ë¶„ì€ Trueë¡œ ì„¤ì • (attentionì—ì„œ ë¬´ì‹œë¨)
            key_padding_mask = (mask == 0)  # [batch_size, seq_len]
        else:
            key_padding_mask = None
        
        attn_output, _ = self.attention(x, x, x, key_padding_mask=key_padding_mask)
        
        # Residual connection + Layer norm
        x = self.norm(x + self.dropout(attn_output))
        
        return x

class KeypointEmbedding(nn.Module):
    """í‚¤í¬ì¸íŠ¸ ì„ë² ë”© ë ˆì´ì–´"""
    
    def __init__(self, input_dim: int, embed_dim: int, dropout: float = 0.1):
        super().__init__()
        self.input_dim = input_dim
        self.embed_dim = embed_dim
        
        # í‚¤í¬ì¸íŠ¸ íŠ¹ì§• ì¶”ì¶œ
        self.projection = nn.Sequential(
            nn.Linear(input_dim, embed_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim)
        )
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ê°€ì¤‘ì¹˜ (ì„ íƒì )
        self.body_part_weights = nn.Parameter(torch.ones(4))  # pose, hand_l, hand_r, face
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, input_dim] í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤
        Returns:
            [batch_size, seq_len, embed_dim] ì„ë² ë”©ëœ ì‹œí€€ìŠ¤
        """
        batch_size, seq_len, _ = x.shape
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ê°€ì¤‘ì¹˜ ì ìš© (ì„ íƒì )
        if self.input_dim == TOTAL_LANDMARKS * 3:  # 1629 = 543 * 3
            # pose(33*3), hand_left(21*3), hand_right(21*3), face(468*3)
            pose_end = 33 * 3
            hand_l_end = pose_end + 21 * 3
            hand_r_end = hand_l_end + 21 * 3
            face_end = hand_r_end + 468 * 3
            
            x_weighted = x.clone()
            x_weighted[:, :, :pose_end] *= self.body_part_weights[0]
            x_weighted[:, :, pose_end:hand_l_end] *= self.body_part_weights[1]
            x_weighted[:, :, hand_l_end:hand_r_end] *= self.body_part_weights[2]
            x_weighted[:, :, hand_r_end:face_end] *= self.body_part_weights[3]
            x = x_weighted
        
        # ì„ë² ë”© íˆ¬ì˜
        embedded = self.projection(x)  # [batch_size, seq_len, embed_dim]
        
        return embedded

class BiLSTMSignClassifier(nn.Module):
    """BiLSTM ê¸°ë°˜ ìˆ˜ì–´ ë¶„ë¥˜ ëª¨ë¸"""
    
    def __init__(self,
                 input_dim: int = TOTAL_LANDMARKS * 3,  # 1629
                 hidden_dim: int = HIDDEN_DIM,           # 256
                 num_layers: int = NUM_LAYERS,           # 2
                 num_classes: int = 1500,                # ì–´íœ˜ í¬ê¸°
                 dropout: float = DROPOUT,               # 0.3
                 use_attention: bool = True,
                 use_positional_encoding: bool = False):
        """
        Args:
            input_dim: ì…ë ¥ í‚¤í¬ì¸íŠ¸ ì°¨ì› (ê¸°ë³¸ 1629)
            hidden_dim: LSTM ìˆ¨ê¹€ì¸µ ì°¨ì›
            num_layers: LSTM ë ˆì´ì–´ ìˆ˜
            num_classes: ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜ (ì–´íœ˜ í¬ê¸°)
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            use_attention: ì–´í…ì…˜ ì‚¬ìš© ì—¬ë¶€
            use_positional_encoding: ìœ„ì¹˜ ì¸ì½”ë”© ì‚¬ìš© ì—¬ë¶€
        """
        super().__init__()
        
        self.logger = setup_logger("BiLSTMSignClassifier")
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.dropout = dropout
        self.use_attention = use_attention
        self.use_positional_encoding = use_positional_encoding
        
        # 1. í‚¤í¬ì¸íŠ¸ ì„ë² ë”©
        self.keypoint_embedding = KeypointEmbedding(
            input_dim=input_dim,
            embed_dim=hidden_dim,
            dropout=dropout
        )
        
        # 2. ìœ„ì¹˜ ì¸ì½”ë”© (ì„ íƒì )
        if self.use_positional_encoding:
            self.pos_encoding = PositionalEncoding(hidden_dim)
        
        # 3. BiLSTM ë ˆì´ì–´ë“¤
        self.lstm_layers = nn.ModuleList()
        for i in range(num_layers):
            layer_input_dim = hidden_dim
            
            lstm_layer = nn.LSTM(
                input_size=layer_input_dim,
                hidden_size=hidden_dim,
                num_layers=1,
                batch_first=True,
                dropout=0 if i == num_layers - 1 else dropout,
                bidirectional=True
            )
            self.lstm_layers.append(lstm_layer)
        
        # BiLSTM ì¶œë ¥ ì°¨ì› (ì–‘ë°©í–¥ì´ë¯€ë¡œ 2ë°°)
        lstm_output_dim = hidden_dim * 2
        
        # 4. ì–´í…ì…˜ ë ˆì´ì–´ (ì„ íƒì )
        if self.use_attention:
            self.attention = AttentionLayer(lstm_output_dim, dropout)
        
        # 5. ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(lstm_output_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
        # 6. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        # 7. ëª¨ë¸ ì •ë³´ ë¡œê¹…
        total_params = count_parameters(self)
        self.logger.info(f"ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ:")
        self.logger.info(f"  - ì…ë ¥ ì°¨ì›: {input_dim}")
        self.logger.info(f"  - ìˆ¨ê¹€ì¸µ ì°¨ì›: {hidden_dim}")
        self.logger.info(f"  - LSTM ë ˆì´ì–´ ìˆ˜: {num_layers}")
        self.logger.info(f"  - ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
        self.logger.info(f"  - ì–´í…ì…˜ ì‚¬ìš©: {use_attention}")
        self.logger.info(f"  - ìœ„ì¹˜ ì¸ì½”ë”© ì‚¬ìš©: {use_positional_encoding}")
        self.logger.info(f"  - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'lstm' in name:
                    # LSTM ê°€ì¤‘ì¹˜ëŠ” Xavier uniform ì´ˆê¸°í™”
                    nn.init.xavier_uniform_(param)
                elif 'linear' in name or 'projection' in name:
                    # Linear ë ˆì´ì–´ëŠ” Kaiming ì´ˆê¸°í™”
                    nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, 
                x: torch.Tensor, 
                mask: Optional[torch.Tensor] = None,
                return_features: bool = False) -> Dict[str, torch.Tensor]:
        """
        Args:
            x: [batch_size, seq_len, input_dim] í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤
            mask: [batch_size, seq_len] íŒ¨ë”© ë§ˆìŠ¤í¬ (1=ì‹¤ì œ ë°ì´í„°, 0=íŒ¨ë”©)
            return_features: ì¤‘ê°„ íŠ¹ì§•ë„ ë°˜í™˜í• ì§€ ì—¬ë¶€
        
        Returns:
            ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì¶œë ¥:
            - 'logits': [batch_size, num_classes] ë¶„ë¥˜ ì ìˆ˜
            - 'features': [batch_size, lstm_output_dim] íŠ¹ì§• ë²¡í„° (ì„ íƒì )
            - 'attention_weights': ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ì„ íƒì )
        """
        batch_size, seq_len, _ = x.shape
        
        # 1. í‚¤í¬ì¸íŠ¸ ì„ë² ë”©
        embedded = self.keypoint_embedding(x)  # [batch_size, seq_len, hidden_dim]
        
        # 2. ìœ„ì¹˜ ì¸ì½”ë”© (ì„ íƒì )
        if self.use_positional_encoding:
            # LSTMì€ batch_first=Trueì´ë¯€ë¡œ transpose í•„ìš”
            embedded = embedded.transpose(0, 1)  # [seq_len, batch_size, hidden_dim]
            embedded = self.pos_encoding(embedded)
            embedded = embedded.transpose(0, 1)  # [batch_size, seq_len, hidden_dim]
        
        # 3. BiLSTM ë ˆì´ì–´ë“¤
        lstm_output = embedded
        
        for i, lstm_layer in enumerate(self.lstm_layers):
            # íŒ¨í‚¹ (íš¨ìœ¨ì ì¸ íŒ¨ë”© ì²˜ë¦¬)
            if mask is not None:
                lengths = mask.sum(dim=1).cpu()  # ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´ë“¤
                packed_input = nn.utils.rnn.pack_padded_sequence(
                    lstm_output, lengths, batch_first=True, enforce_sorted=False
                )
                packed_output, _ = lstm_layer(packed_input)
                lstm_output, _ = nn.utils.rnn.pad_packed_sequence(
                    packed_output, batch_first=True
                )
            else:
                lstm_output, _ = lstm_layer(lstm_output)
            
            # ì¤‘ê°„ ë ˆì´ì–´ì— ëŒ€í•´ì„œëŠ” ì”ì°¨ ì—°ê²° (ì„ íƒì )
            if i > 0 and lstm_output.size(-1) == embedded.size(-1):
                lstm_output = lstm_output + embedded
        
        # [batch_size, seq_len, hidden_dim * 2]
        
        # 4. ì–´í…ì…˜ (ì„ íƒì )
        if self.use_attention:
            # ì–´í…ì…˜ì„ ìœ„í•´ transpose
            lstm_output_T = lstm_output.transpose(0, 1)  # [seq_len, batch_size, hidden_dim*2]
            attended_output = self.attention(lstm_output_T, mask)
            lstm_output = attended_output.transpose(0, 1)  # [batch_size, seq_len, hidden_dim*2]
        
        # 5. ì „ì—­ í’€ë§ (ì‹œí€€ìŠ¤ â†’ ê³ ì • í¬ê¸° ë²¡í„°)
        if mask is not None:
            # ë§ˆìŠ¤í‚¹ëœ í‰ê·  í’€ë§
            mask_expanded = mask.unsqueeze(-1).float()  # [batch_size, seq_len, 1]
            masked_output = lstm_output * mask_expanded
            sequence_lengths = mask.sum(dim=1, keepdim=True).float()  # [batch_size, 1]
            pooled_output = masked_output.sum(dim=1) / (sequence_lengths + 1e-8)  # [batch_size, hidden_dim*2]
        else:
            # ë‹¨ìˆœ í‰ê·  í’€ë§
            pooled_output = lstm_output.mean(dim=1)  # [batch_size, hidden_dim*2]
        
        # 6. ë¶„ë¥˜
        logits = self.classifier(pooled_output)  # [batch_size, num_classes]
        
        # 7. ê²°ê³¼ êµ¬ì„±
        outputs = {'logits': logits}
        
        if return_features:
            outputs['features'] = pooled_output
        
        return outputs
    
    def predict(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ì¶”ë¡ ìš© í•¨ìˆ˜ (í™•ë¥ ê°’ ë°˜í™˜)"""
        self.eval()
        with torch.no_grad():
            outputs = self.forward(x, mask)
            probabilities = F.softmax(outputs['logits'], dim=-1)
        return probabilities
    
    def get_predictions(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ì˜ˆì¸¡ í´ë˜ìŠ¤ì™€ í™•ë¥  ë°˜í™˜"""
        probabilities = self.predict(x, mask)
        predicted_classes = torch.argmax(probabilities, dim=-1)
        max_probabilities = torch.max(probabilities, dim=-1)[0]
        
        return predicted_classes, max_probabilities

class SignLanguageModel(nn.Module):
    """ì „ì²´ ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, vocab_size: int, **model_kwargs):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.model = BiLSTMSignClassifier(num_classes=vocab_size, **model_kwargs)
        
    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ë°ì´í„°ë¡œë¶€í„° ì˜ˆì¸¡"""
        sequences = batch['sequence']          # [batch_size, seq_len, input_dim]
        masks = batch.get('sequence_mask')     # [batch_size, seq_len]
        
        outputs = self.model(sequences, masks)
        
        # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš° ì†ì‹¤ ê³„ì‚°
        if 'label' in batch:
            labels = batch['label']  # [batch_size]
            loss = F.cross_entropy(outputs['logits'], labels)
            outputs['loss'] = loss
        
        return outputs
    
    def predict_batch(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ì˜ˆì¸¡ (í™•ë¥ ê°’ í¬í•¨)"""
        self.eval()
        with torch.no_grad():
            sequences = batch['sequence']
            masks = batch.get('sequence_mask')
            
            outputs = self.model(sequences, masks, return_features=True)
            probabilities = F.softmax(outputs['logits'], dim=-1)
            predicted_classes = torch.argmax(probabilities, dim=-1)
            max_probabilities = torch.max(probabilities, dim=-1)[0]
            
            return {
                'predictions': predicted_classes,
                'probabilities': probabilities,
                'max_probabilities': max_probabilities,
                'features': outputs.get('features')
            }

def create_model(vocab_size: int, device: Optional[torch.device] = None, **kwargs) -> SignLanguageModel:
    """ëª¨ë¸ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    model = SignLanguageModel(vocab_size=vocab_size, **kwargs)
    
    if device is not None:
        model = model.to(device)
    
    return model

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§ª ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ìƒì„±
    vocab_size = 1504  # 1500 ë‹¨ì–´ + 4 íŠ¹ìˆ˜í† í°
    model = create_model(vocab_size=vocab_size)
    
    print(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ: {count_parameters(model):,} íŒŒë¼ë¯¸í„°")
    
    # ë”ë¯¸ ë°ì´í„° í…ŒìŠ¤íŠ¸
    batch_size = 8
    seq_len = 200
    input_dim = TOTAL_LANDMARKS * 3  # 1629
    
    # ë”ë¯¸ ë°°ì¹˜ ìƒì„±
    dummy_batch = {
        'sequence': torch.randn(batch_size, seq_len, input_dim),
        'sequence_mask': torch.ones(batch_size, seq_len),
        'label': torch.randint(0, vocab_size, (batch_size,))
    }
    
    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸:")
    model.train()
    outputs = model(dummy_batch)
    
    print(f"  ë¡œì§“ í¬ê¸°: {outputs['logits'].shape}")
    print(f"  ì†ì‹¤ê°’: {outputs['loss'].item():.4f}")
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
    pred_outputs = model.predict_batch(dummy_batch)
    
    print(f"  ì˜ˆì¸¡ í´ë˜ìŠ¤: {pred_outputs['predictions'][:5]}")
    print(f"  ìµœëŒ€ í™•ë¥ : {pred_outputs['max_probabilities'][:5]}")
    print(f"  íŠ¹ì§• í¬ê¸°: {pred_outputs['features'].shape}")
    
    print("\nâœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
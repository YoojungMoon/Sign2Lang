#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SU:DA - ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸
BiLSTM ê¸°ë°˜ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ (ì•ˆì „ ì´ˆê¸°í™” ì ìš©íŒ)
"""

from typing import Dict, Tuple, Optional
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

from utils import (
    setup_logger, KEYPOINT_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT,
    TOTAL_LANDMARKS, count_parameters
)

# =========================
# Positional Encoding
# =========================
class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”© (ì„ íƒì  ì‚¬ìš©)"""
    def __init__(self, d_model: int, max_len: int = 200):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # [max_len, 1, d_model]
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [seq_len, batch, d_model]
        """
        return x + self.pe[:x.size(0), :]


# =========================
# Attention Layer
# =========================
class AttentionLayer(nn.Module):
    """Multi-Head Self-Attention + Residual + LayerNorm"""
    def __init__(self, model_dim: int, dropout: float = 0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(
            embed_dim=model_dim, num_heads=8, dropout=dropout, batch_first=False
        )
        self.norm = nn.LayerNorm(model_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        x:    [seq_len, batch, model_dim]
        mask: [batch, seq_len] (1=valid, 0=pad)
        """
        key_padding_mask = (mask == 0) if mask is not None else None
        out, _ = self.attn(x, x, x, key_padding_mask=key_padding_mask)
        return self.norm(x + self.dropout(out))


# =========================
# Keypoint Embedding
# =========================
class KeypointEmbedding(nn.Module):
    """í‚¤í¬ì¸íŠ¸ ì„ë² ë”© ë ˆì´ì–´"""
    def __init__(self, input_dim: int, embed_dim: int, dropout: float = 0.1):
        super().__init__()
        self.input_dim = input_dim
        self.embed_dim = embed_dim

        self.projection = nn.Sequential(
            nn.Linear(input_dim, embed_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim)  # <- 1D íŒŒë¼ë¯¸í„° (ì•ˆì „ ì´ˆê¸°í™” í•„ìš”)
        )

        # ì‹ ì²´ ë¶€ìœ„ë³„ ê°€ì¤‘ì¹˜(ì„ íƒ)
        self.body_part_weights = nn.Parameter(torch.ones(4))  # pose, hand_l, hand_r, face

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [batch, seq_len, input_dim]
        return: [batch, seq_len, embed_dim]
        """
        if self.input_dim == TOTAL_LANDMARKS * 3:
            # pose(33*3), hand_l(21*3), hand_r(21*3), face(468*3)
            pose_end = 33 * 3
            hand_l_end = pose_end + 21 * 3
            hand_r_end = hand_l_end + 21 * 3
            face_end = hand_r_end + 468 * 3

            x = x.clone()
            x[:, :, :pose_end] *= self.body_part_weights[0]
            x[:, :, pose_end:hand_l_end] *= self.body_part_weights[1]
            x[:, :, hand_l_end:hand_r_end] *= self.body_part_weights[2]
            x[:, :, hand_r_end:face_end] *= self.body_part_weights[3]

        return self.projection(x)


# =========================
# BiLSTM Classifier
# =========================
class BiLSTMSignClassifier(nn.Module):
    """BiLSTM ê¸°ë°˜ ìˆ˜ì–´ ë¶„ë¥˜ ëª¨ë¸"""
    def __init__(self,
                 input_dim: int = TOTAL_LANDMARKS * 3,   # 1629
                 hidden_dim: int = HIDDEN_DIM,           # 256
                 num_layers: int = NUM_LAYERS,           # 2
                 num_classes: int = 1500,                # ì–´íœ˜ í¬ê¸°
                 dropout: float = DROPOUT,               # 0.3
                 use_attention: bool = True,
                 use_positional_encoding: bool = False):
        super().__init__()

        self.logger = setup_logger("BiLSTMSignClassifier")
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.dropout = dropout
        self.use_attention = use_attention
        self.use_positional_encoding = use_positional_encoding

        # 1) í‚¤í¬ì¸íŠ¸ ì„ë² ë”©
        self.keypoint_embedding = KeypointEmbedding(
            input_dim=input_dim, embed_dim=hidden_dim, dropout=dropout
        )

        # 2) ìœ„ì¹˜ ì¸ì½”ë”©(ì˜µì…˜)
        if self.use_positional_encoding:
            self.pos_encoding = PositionalEncoding(hidden_dim)

        # 3) BiLSTM ìŠ¤íƒ (í•œ ì¸µì§œë¦¬ LSTMì„ num_layersë²ˆ ìŠ¤íƒ)
        #    ë‚´ë¶€ LSTMì˜ dropoutì€ num_layers==1ì¼ ë•Œ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ ê²½ê³  ë°©ì§€ ì°¨ì›ì—ì„œ 0.0ìœ¼ë¡œ ê³ ì •
        self.lstm_layers = nn.ModuleList()
        for _ in range(num_layers):
            self.lstm_layers.append(nn.LSTM(
                input_size=hidden_dim,
                hidden_size=hidden_dim,
                num_layers=1,
                batch_first=True,
                dropout=0.0,           # <-- ê²½ê³  ì œê±°
                bidirectional=True
            ))

        lstm_output_dim = hidden_dim * 2  # ì–‘ë°©í–¥

        # 4) ì–´í…ì…˜(ì˜µì…˜)
        if self.use_attention:
            self.attention = AttentionLayer(lstm_output_dim, dropout)

        # 5) ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(lstm_output_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

        # 6) ì•ˆì „ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ëª¨ë“ˆ íƒ€ì… ê¸°ë°˜)
        self._initialize_weights()

        # 7) ë¡œê¹…
        self.logger.info("ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ:")
        self.logger.info(f"  - ì…ë ¥ ì°¨ì›: {input_dim}")
        self.logger.info(f"  - ìˆ¨ê¹€ì¸µ ì°¨ì›: {hidden_dim}")
        self.logger.info(f"  - LSTM ë ˆì´ì–´ ìˆ˜: {num_layers}")
        self.logger.info(f"  - ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
        self.logger.info(f"  - ì–´í…ì…˜ ì‚¬ìš©: {use_attention}")
        self.logger.info(f"  - ìœ„ì¹˜ ì¸ì½”ë”© ì‚¬ìš©: {use_positional_encoding}")
        self.logger.info(f"  - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {count_parameters(self):,}")

    # -------- ì•ˆì „ ì´ˆê¸°í™” (ê¶Œì¥ì•ˆ) --------
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # Linear: 2D weight â†’ fan-in/out ê°€ëŠ¥, biasëŠ” 1D â†’ zeros
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

            elif isinstance(m, nn.LayerNorm):
                # LayerNorm: 1D íŒŒë¼ë¯¸í„° â†’ fan-in/out ì‚¬ìš© ê¸ˆì§€
                if m.elementwise_affine:
                    nn.init.ones_(m.weight)
                    nn.init.zeros_(m.bias)

            elif isinstance(m, nn.LSTM):
                # LSTMì€ íŒŒë¼ë¯¸í„°ë³„ë¡œ ì•ˆì „ ì´ˆê¸°í™”
                for name, p in m.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(p)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(p)
                    elif 'bias' in name:
                        nn.init.zeros_(p)
                        # forget gate bias boost (ì„ íƒ)
                        n = p.size(0)
                        p.data[n // 4:n // 2].fill_(1.0)

    # -------- Forward --------
    def forward(self,
                x: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                return_features: bool = False) -> Dict[str, torch.Tensor]:
        """
        x:    [batch, seq_len, input_dim]
        mask: [batch, seq_len] (1=valid, 0=pad)
        """
        # 1) ì„ë² ë”©
        embedded = self.keypoint_embedding(x)  # [B, T, H]

        # 2) ìœ„ì¹˜ ì¸ì½”ë”©(ì˜µì…˜)
        if self.use_positional_encoding:
            embedded = embedded.transpose(0, 1)      # [T, B, H]
            embedded = self.pos_encoding(embedded)
            embedded = embedded.transpose(0, 1)      # [B, T, H]

        # 3) BiLSTM ìŠ¤íƒ
        out = embedded
        for i, lstm in enumerate(self.lstm_layers):
            if mask is not None:
                lengths = mask.sum(dim=1).cpu()
                packed = nn.utils.rnn.pack_padded_sequence(out, lengths, batch_first=True, enforce_sorted=False)
                packed_out, _ = lstm(packed)
                out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)
            else:
                out, _ = lstm(out)

            # (ì„ íƒ) ë™ì¼ ì°¨ì›ì¼ ë•Œ ì”ì°¨ ì—°ê²°
            if i > 0 and out.size(-1) == embedded.size(-1):
                out = out + embedded

        # 4) ì–´í…ì…˜(ì˜µì…˜)
        if self.use_attention:
            out_T = out.transpose(0, 1)                # [T, B, 2H]
            out_T = self.attention(out_T, mask)        # [T, B, 2H]
            out = out_T.transpose(0, 1)                # [B, T, 2H]

        # 5) ë§ˆìŠ¤í‚¹ í‰ê·  í’€ë§
        if mask is not None:
            m = mask.unsqueeze(-1).float()             # [B, T, 1]
            summed = (out * m).sum(dim=1)              # [B, 2H]
            lengths = mask.sum(dim=1, keepdim=True).float()
            pooled = summed / (lengths + 1e-8)
        else:
            pooled = out.mean(dim=1)                   # [B, 2H]

        # 6) ë¶„ë¥˜
        logits = self.classifier(pooled)               # [B, C]

        outputs = {'logits': logits}
        if return_features:
            outputs['features'] = pooled
        return outputs

    # -------- Helper (inference) --------
    def predict(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        self.eval()
        with torch.no_grad():
            probs = F.softmax(self.forward(x, mask)['logits'], dim=-1)
        return probs

    def get_predictions(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        probs = self.predict(x, mask)
        preds = torch.argmax(probs, dim=-1)
        maxp = torch.max(probs, dim=-1)[0]
        return preds, maxp


# =========================
# Model Wrapper
# =========================
class SignLanguageModel(nn.Module):
    """ë°°ì¹˜ dict ì…ì¶œë ¥ ë˜í¼(í•™ìŠµ ë£¨í”„ì™€ ë§ë¬¼ë¦¼)"""
    def __init__(self, vocab_size: int, **model_kwargs):
        super().__init__()
        self.vocab_size = vocab_size
        self.model = BiLSTMSignClassifier(num_classes=vocab_size, **model_kwargs)

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        seqs = batch['sequence']                  # [B, T, D]
        masks = batch.get('sequence_mask')        # [B, T]
        out = self.model(seqs, masks)
        if 'label' in batch:
            labels = batch['label']               # [B]
            out['loss'] = F.cross_entropy(out['logits'], labels)
        return out

    def predict_batch(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        self.eval()
        with torch.no_grad():
            seqs = batch['sequence']
            masks = batch.get('sequence_mask')
            out = self.model(seqs, masks, return_features=True)
            probs = F.softmax(out['logits'], dim=-1)
            preds = torch.argmax(probs, dim=-1)
            maxp = torch.max(probs, dim=-1)[0]
            return {
                'predictions': preds,
                'probabilities': probs,
                'max_probabilities': maxp,
                'features': out.get('features')
            }


# =========================
# Factory
# =========================
def create_model(vocab_size: int, device: Optional[torch.device] = None, **kwargs) -> SignLanguageModel:
    model = SignLanguageModel(vocab_size=vocab_size, **kwargs)
    if device is not None:
        model = model.to(device)
    return model


# =========================
# Quick self test
# =========================
def main():
    print("ğŸ§ª ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    vocab_size = 1504  # ì˜ˆ: 1500 ë‹¨ì–´ + íŠ¹ìˆ˜í† í°
    model = create_model(vocab_size=vocab_size)
    print(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ: {count_parameters(model):,} íŒŒë¼ë¯¸í„°")

    # ë”ë¯¸ ë°°ì¹˜
    B, T, D = 4, 120, TOTAL_LANDMARKS * 3
    batch = {
        'sequence': torch.randn(B, T, D),
        'sequence_mask': torch.ones(B, T),
        'label': torch.randint(0, vocab_size, (B,))
    }

    # ìˆœì „íŒŒ
    out = model(batch)
    print(f"ë¡œì§“ í¬ê¸°: {out['logits'].shape}, ì†ì‹¤: {out['loss'].item():.4f}")

    # ì˜ˆì¸¡
    pred_out = model.predict_batch(batch)
    print(f"ì˜ˆì¸¡: {pred_out['predictions']}, ìµœëŒ€í™•ë¥ : {pred_out['max_probabilities']}")

    print("âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    main()

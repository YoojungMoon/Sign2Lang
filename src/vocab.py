#!/usr/bin/env python3
"""
SU:DA - ìˆ˜ì–´ ë‹¨ì–´ ì‚¬ì „ ê´€ë¦¬ ëª¨ë“ˆ
ìˆ˜ì–´ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ ê°„ì˜ ë§¤í•‘ì„ ê´€ë¦¬
"""

import json
from pathlib import Path
from typing import Dict, List, Optional, Union
from utils import setup_logger, load_json, save_json, DATA_PROCESSED_PATH

class SignVocabulary:
    """ìˆ˜ì–´ ë‹¨ì–´ ì‚¬ì „ í´ë˜ìŠ¤"""
    
    def __init__(self, vocab_path: Optional[Path] = None):
        """
        Args:
            vocab_path: ì‚¬ì „ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        """
        self.logger = setup_logger("SignVocabulary")
        
        # ê¸°ë³¸ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
        if vocab_path is None:
            vocab_path = DATA_PROCESSED_PATH / "vocab" / "vocab.json"
        
        self.vocab_path = vocab_path
        
        # íŠ¹ìˆ˜ í† í° ì •ì˜
        self.special_tokens = {
            "PAD": "<PAD>",      # íŒ¨ë”© í† í°
            "UNK": "<UNK>",      # ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´
            "SOS": "<SOS>",      # ì‹œì‘ í† í° (Start of Sequence)
            "EOS": "<EOS>"       # ì¢…ë£Œ í† í° (End of Sequence)
        }
        
        # ì‚¬ì „ ì´ˆê¸°í™”
        self.word_to_idx: Dict[str, int] = {}
        self.idx_to_word: Dict[int, str] = {}
        self.vocab_size: int = 0
        
        # ì‚¬ì „ ë¡œë”©
        self.load_vocabulary()
    
    def load_vocabulary(self) -> bool:
        """ì‚¬ì „ íŒŒì¼ì—ì„œ ë‹¨ì–´ ì‚¬ì „ ë¡œë”©"""
        if not self.vocab_path.exists():
            self.logger.warning(f"ì‚¬ì „ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.vocab_path}")
            self.logger.info("ë¹ˆ ì‚¬ì „ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self._initialize_empty_vocab()
            return False
        
        try:
            vocab_data = load_json(self.vocab_path)
            
            # word -> idx ë§¤í•‘
            self.word_to_idx = vocab_data
            
            # idx -> word ë§¤í•‘ (ì—­ë°©í–¥)
            self.idx_to_word = {idx: word for word, idx in vocab_data.items()}
            
            # ì‚¬ì „ í¬ê¸°
            self.vocab_size = len(vocab_data)
            
            self.logger.info(f"ì‚¬ì „ ë¡œë”© ì™„ë£Œ: {self.vocab_size}ê°œ ë‹¨ì–´")
            self.logger.info(f"íŠ¹ìˆ˜ í† í°: {list(self.special_tokens.values())}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._initialize_empty_vocab()
            return False
    
    def _initialize_empty_vocab(self):
        """ë¹ˆ ì‚¬ì „ìœ¼ë¡œ ì´ˆê¸°í™” (íŠ¹ìˆ˜ í† í°ë§Œ)"""
        self.word_to_idx = {
            self.special_tokens["PAD"]: 0,
            self.special_tokens["UNK"]: 1,
            self.special_tokens["SOS"]: 2,
            self.special_tokens["EOS"]: 3
        }
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.vocab_size = len(self.word_to_idx)
    
    def save_vocabulary(self, save_path: Optional[Path] = None):
        """ì‚¬ì „ì„ íŒŒì¼ë¡œ ì €ì¥"""
        if save_path is None:
            save_path = self.vocab_path
        
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            # JSON í˜•íƒœë¡œ ì €ì¥
            save_json(self.word_to_idx, save_path)
            
            # í…ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ì €ì¥
            txt_path = save_path.with_suffix('.txt')
            with open(txt_path, 'w', encoding='utf-8') as f:
                for word, idx in sorted(self.word_to_idx.items(), key=lambda x: x[1]):
                    f.write(f"{idx}\t{word}\n")
            
            self.logger.info(f"ì‚¬ì „ ì €ì¥ ì™„ë£Œ: {save_path}")
            
        except Exception as e:
            self.logger.error(f"ì‚¬ì „ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def add_word(self, word: str) -> int:
        """ìƒˆ ë‹¨ì–´ë¥¼ ì‚¬ì „ì— ì¶”ê°€"""
        if word in self.word_to_idx:
            return self.word_to_idx[word]
        
        # ìƒˆ ì¸ë±ìŠ¤ í• ë‹¹
        new_idx = self.vocab_size
        self.word_to_idx[word] = new_idx
        self.idx_to_word[new_idx] = word
        self.vocab_size += 1
        
        self.logger.debug(f"ìƒˆ ë‹¨ì–´ ì¶”ê°€: '{word}' -> {new_idx}")
        
        return new_idx
    
    def add_words(self, words: List[str]) -> List[int]:
        """ì—¬ëŸ¬ ë‹¨ì–´ë¥¼ í•œë²ˆì— ì¶”ê°€"""
        indices = []
        for word in words:
            idx = self.add_word(word)
            indices.append(idx)
        
        self.logger.info(f"{len(words)}ê°œ ë‹¨ì–´ ì¶”ê°€ ì™„ë£Œ")
        return indices
    
    def word_to_index(self, word: str) -> int:
        """ë‹¨ì–´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
        return self.word_to_idx.get(word, self.word_to_idx[self.special_tokens["UNK"]])
    
    def index_to_word(self, index: int) -> str:
        """ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜"""
        return self.idx_to_word.get(index, self.special_tokens["UNK"])
    
    def words_to_indices(self, words: List[str]) -> List[int]:
        """ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return [self.word_to_index(word) for word in words]
    
    def indices_to_words(self, indices: List[int]) -> List[str]:
        """ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return [self.index_to_word(idx) for idx in indices]
    
    def encode(self, text: Union[str, List[str]], 
               add_special_tokens: bool = False) -> List[int]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ì¸ì½”ë”©
        
        Args:
            text: ë‹¨ì¼ ë‹¨ì–´(str) ë˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸(List[str])
            add_special_tokens: SOS, EOS í† í° ì¶”ê°€ ì—¬ë¶€
        
        Returns:
            ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if isinstance(text, str):
            words = [text]
        else:
            words = text
        
        indices = self.words_to_indices(words)
        
        if add_special_tokens:
            sos_idx = self.word_to_idx[self.special_tokens["SOS"]]
            eos_idx = self.word_to_idx[self.special_tokens["EOS"]]
            indices = [sos_idx] + indices + [eos_idx]
        
        return indices
    
    def decode(self, indices: List[int], 
               remove_special_tokens: bool = True) -> List[str]:
        """
        ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        
        Args:
            indices: ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            remove_special_tokens: íŠ¹ìˆ˜ í† í° ì œê±° ì—¬ë¶€
        
        Returns:
            ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        """
        words = self.indices_to_words(indices)
        
        if remove_special_tokens:
            special_token_values = set(self.special_tokens.values())
            words = [word for word in words if word not in special_token_values]
        
        return words
    
    def contains_word(self, word: str) -> bool:
        """ë‹¨ì–´ê°€ ì‚¬ì „ì— ìˆëŠ”ì§€ í™•ì¸"""
        return word in self.word_to_idx
    
    def contains_index(self, index: int) -> bool:
        """ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        return index in self.idx_to_word
    
    def get_vocab_size(self) -> int:
        """ì‚¬ì „ í¬ê¸° ë°˜í™˜"""
        return self.vocab_size
    
    def get_special_token_idx(self, token_name: str) -> int:
        """íŠ¹ìˆ˜ í† í°ì˜ ì¸ë±ìŠ¤ ë°˜í™˜"""
        if token_name not in self.special_tokens:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” íŠ¹ìˆ˜ í† í°: {token_name}")
        
        token = self.special_tokens[token_name]
        return self.word_to_idx[token]
    
    def get_pad_idx(self) -> int:
        """PAD í† í° ì¸ë±ìŠ¤ ë°˜í™˜"""
        return self.get_special_token_idx("PAD")
    
    def get_unk_idx(self) -> int:
        """UNK í† í° ì¸ë±ìŠ¤ ë°˜í™˜"""
        return self.get_special_token_idx("UNK")
    
    def get_sos_idx(self) -> int:
        """SOS í† í° ì¸ë±ìŠ¤ ë°˜í™˜"""
        return self.get_special_token_idx("SOS")
    
    def get_eos_idx(self) -> int:
        """EOS í† í° ì¸ë±ìŠ¤ ë°˜í™˜"""
        return self.get_special_token_idx("EOS")
    
    def get_word_list(self, include_special_tokens: bool = True) -> List[str]:
        """ëª¨ë“  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        if include_special_tokens:
            return list(self.word_to_idx.keys())
        else:
            special_token_values = set(self.special_tokens.values())
            return [word for word in self.word_to_idx.keys() 
                    if word not in special_token_values]
    
    def get_word_frequencies(self, word_list: List[str]) -> Dict[str, int]:
        """ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°"""
        freq = {}
        for word in word_list:
            freq[word] = freq.get(word, 0) + 1
        return freq
    
    def filter_by_frequency(self, word_list: List[str], 
                          min_freq: int = 1) -> List[str]:
        """ë¹ˆë„ê°€ ì„ê³„ê°’ ì´ìƒì¸ ë‹¨ì–´ë“¤ë§Œ í•„í„°ë§"""
        frequencies = self.get_word_frequencies(word_list)
        return [word for word, freq in frequencies.items() if freq >= min_freq]
    
    def print_vocabulary_info(self):
        """ì‚¬ì „ ì •ë³´ ì¶œë ¥"""
        print("=" * 50)
        print("ğŸ“š ìˆ˜ì–´ ë‹¨ì–´ ì‚¬ì „ ì •ë³´")
        print("=" * 50)
        print(f"ğŸ“Š ì´ ë‹¨ì–´ ìˆ˜: {self.vocab_size}")
        print(f"ğŸ”¤ íŠ¹ìˆ˜ í† í° ìˆ˜: {len(self.special_tokens)}")
        print(f"ğŸ’¬ ì¼ë°˜ ë‹¨ì–´ ìˆ˜: {self.vocab_size - len(self.special_tokens)}")
        print()
        print("ğŸ·ï¸ íŠ¹ìˆ˜ í† í°:")
        for name, token in self.special_tokens.items():
            idx = self.word_to_idx[token]
            print(f"  {name}: '{token}' -> {idx}")
        print()
        
        # ì¼ë°˜ ë‹¨ì–´ ëª‡ ê°œ ì˜ˆì‹œ
        regular_words = self.get_word_list(include_special_tokens=False)
        if regular_words:
            print("ğŸ“ ìˆ˜ì–´ ë‹¨ì–´ ì˜ˆì‹œ (ì²˜ìŒ 10ê°œ):")
            for i, word in enumerate(regular_words[:10]):
                idx = self.word_to_idx[word]
                print(f"  {word} -> {idx}")
            if len(regular_words) > 10:
                print(f"  ... ì™¸ {len(regular_words) - 10}ê°œ")
        print("=" * 50)
    
    def __len__(self) -> int:
        """ì‚¬ì „ í¬ê¸° ë°˜í™˜ (len() í•¨ìˆ˜ìš©)"""
        return self.vocab_size
    
    def __contains__(self, item) -> bool:
        """ë‹¨ì–´/ì¸ë±ìŠ¤ í¬í•¨ ì—¬ë¶€ í™•ì¸ (in ì—°ì‚°ììš©)"""
        if isinstance(item, str):
            return self.contains_word(item)
        elif isinstance(item, int):
            return self.contains_index(item)
        else:
            return False
    
    def __getitem__(self, key):
        """ì¸ë±ì‹± ì ‘ê·¼ (vocab[word] ë˜ëŠ” vocab[index])"""
        if isinstance(key, str):
            return self.word_to_index(key)
        elif isinstance(key, int):
            return self.index_to_word(key)
        else:
            raise TypeError("í‚¤ëŠ” ë¬¸ìì—´(ë‹¨ì–´) ë˜ëŠ” ì •ìˆ˜(ì¸ë±ìŠ¤)ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    def __repr__(self) -> str:
        return f"SignVocabulary(size={self.vocab_size}, path='{self.vocab_path}')"

def create_vocabulary_from_words(words: List[str], 
                                save_path: Optional[Path] = None) -> SignVocabulary:
    """
    ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ìƒˆë¡œìš´ ì‚¬ì „ ìƒì„±
    
    Args:
        words: ìˆ˜ì–´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        save_path: ì €ì¥í•  ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ)
    
    Returns:
        ìƒì„±ëœ SignVocabulary ê°ì²´
    """
    # ë¹ˆ ì‚¬ì „ ìƒì„±
    vocab = SignVocabulary(vocab_path=save_path)
    
    # ë‹¨ì–´ë“¤ ì¶”ê°€
    unique_words = sorted(set(words))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    vocab.add_words(unique_words)
    
    # ì €ì¥
    if save_path:
        vocab.save_vocabulary(save_path)
    
    return vocab

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    # ê¸°ë³¸ ì‚¬ì „ ë¡œë”© í…ŒìŠ¤íŠ¸
    vocab = SignVocabulary()
    
    # ì‚¬ì „ ì •ë³´ ì¶œë ¥
    vocab.print_vocabulary_info()
    
    # ê¸°ë³¸ ì‚¬ìš©ë²• í…ŒìŠ¤íŠ¸
    if vocab.vocab_size > 4:  # íŠ¹ìˆ˜ í† í° ì™¸ì— ì¼ë°˜ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°
        print("\nğŸ§ª ì‚¬ìš©ë²• í…ŒìŠ¤íŠ¸:")
        
        # ì²« ë²ˆì§¸ ì¼ë°˜ ë‹¨ì–´ë¡œ í…ŒìŠ¤íŠ¸
        regular_words = vocab.get_word_list(include_special_tokens=False)
        if regular_words:
            test_word = regular_words[0]
            
            # ì¸ì½”ë”©/ë””ì½”ë”© í…ŒìŠ¤íŠ¸
            encoded = vocab.encode(test_word)
            decoded = vocab.decode(encoded)
            
            print(f"ë‹¨ì–´: '{test_word}'")
            print(f"ì¸ì½”ë”©: {encoded}")
            print(f"ë””ì½”ë”©: {decoded}")
            
            # íŠ¹ìˆ˜ í† í° í¬í•¨ í…ŒìŠ¤íŠ¸
            encoded_with_special = vocab.encode(test_word, add_special_tokens=True)
            decoded_with_special = vocab.decode(encoded_with_special, remove_special_tokens=False)
            
            print(f"íŠ¹ìˆ˜í† í° í¬í•¨ ì¸ì½”ë”©: {encoded_with_special}")
            print(f"íŠ¹ìˆ˜í† í° í¬í•¨ ë””ì½”ë”©: {decoded_with_special}")

if __name__ == "__main__":
    main()
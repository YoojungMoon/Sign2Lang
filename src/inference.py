#!/usr/bin/env python3
"""
SU:DA - ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì¶”ë¡ 
ì›¹ìº ì„ í†µí•œ ì‹¤ì‹œê°„ ìˆ˜ì–´ ë™ì‘ ì¸ì‹ ë° í…ìŠ¤íŠ¸ ë³€í™˜
"""

import cv2
import torch
import numpy as np
import mediapipe as mp
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Deque
from collections import deque
import time
import threading
import queue
from datetime import datetime

from utils import (
    setup_logger, load_checkpoint, get_device, normalize_keypoints,
    pad_sequence, CHECKPOINTS_PATH, TOTAL_LANDMARKS
)
from model import create_model
from vocab import SignVocabulary

class MediaPipeExtractor:
    """MediaPipeë¥¼ ì´ìš©í•œ ì‹¤ì‹œê°„ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
    
    def __init__(self):
        self.logger = setup_logger("MediaPipeExtractor")
        
        # MediaPipe ì´ˆê¸°í™”
        self.mp_holistic = mp.solutions.holistic
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # Holistic ëª¨ë¸ ì´ˆê¸°í™”
        self.holistic = self.mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=1,
            smooth_landmarks=True,
            enable_segmentation=False,
            smooth_segmentation=False,
            refine_face_landmarks=True,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        
        self.logger.info("MediaPipe Holistic ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_keypoints(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        # BGRì„ RGBë¡œ ë³€í™˜
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        rgb_image.flags.writeable = False
        
        # MediaPipe ì²˜ë¦¬
        results = self.holistic.process(rgb_image)
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = self._extract_landmarks(results)
        
        return keypoints, results
    
    def _extract_landmarks(self, results) -> np.ndarray:
        """ëœë“œë§ˆí¬ ê²°ê³¼ì—ì„œ í‚¤í¬ì¸íŠ¸ ë°°ì—´ ì¶”ì¶œ"""
        keypoints = []
        
        # Pose landmarks (33ê°œ)
        if results.pose_landmarks:
            for landmark in results.pose_landmarks.landmark:
                keypoints.extend([landmark.x, landmark.y, landmark.visibility])
        else:
            keypoints.extend([0.0] * (33 * 3))
        
        # Left hand landmarks (21ê°œ)
        if results.left_hand_landmarks:
            for landmark in results.left_hand_landmarks.landmark:
                keypoints.extend([landmark.x, landmark.y, landmark.visibility])
        else:
            keypoints.extend([0.0] * (21 * 3))
        
        # Right hand landmarks (21ê°œ)
        if results.right_hand_landmarks:
            for landmark in results.right_hand_landmarks.landmark:
                keypoints.extend([landmark.x, landmark.y, landmark.visibility])
        else:
            keypoints.extend([0.0] * (21 * 3))
        
        # Face landmarks (468ê°œ)
        if results.face_landmarks:
            for landmark in results.face_landmarks.landmark:
                keypoints.extend([landmark.x, landmark.y, landmark.visibility])
        else:
            keypoints.extend([0.0] * (468 * 3))
        
        return np.array(keypoints, dtype=np.float32)
    
    def draw_landmarks(self, image: np.ndarray, results) -> np.ndarray:
        """ì´ë¯¸ì§€ì— ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°"""
        image.flags.writeable = True
        
        # Pose
        if results.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                image, results.pose_landmarks, self.mp_holistic.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()
            )
        
        # Hands
        if results.left_hand_landmarks:
            self.mp_drawing.draw_landmarks(
                image, results.left_hand_landmarks, self.mp_holistic.HAND_CONNECTIONS,
                self.mp_drawing_styles.get_default_hand_landmarks_style(),
                self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        if results.right_hand_landmarks:
            self.mp_drawing.draw_landmarks(
                image, results.right_hand_landmarks, self.mp_holistic.HAND_CONNECTIONS,
                self.mp_drawing_styles.get_default_hand_landmarks_style(),
                self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        # Face (ì„ íƒì  - ë„ˆë¬´ ë§ì•„ì„œ ìƒëµ ê°€ëŠ¥)
        # if results.face_landmarks:
        #     self.mp_drawing.draw_landmarks(
        #         image, results.face_landmarks, self.mp_holistic.FACEMESH_CONTOURS,
        #         None, self.mp_drawing_styles.get_default_face_mesh_contours_style()
        #     )
        
        return image
    
    def __del__(self):
        """ì†Œë©¸ì"""
        if hasattr(self, 'holistic'):
            self.holistic.close()

class SignLanguageInference:
    """ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì¶”ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self,
                 checkpoint_path: Optional[Path] = None,
                 vocab_path: Optional[Path] = None,
                 sequence_length: int = 60,
                 confidence_threshold: float = 0.7,
                 detection_cooldown: float = 2.0,
                 device: Optional[torch.device] = None):
        """
        Args:
            checkpoint_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            vocab_path: ì‚¬ì „ íŒŒì¼ ê²½ë¡œ  
            sequence_length: ë¶„ì„í•  ì‹œí€€ìŠ¤ ê¸¸ì´ (í”„ë ˆì„ ìˆ˜)
            confidence_threshold: ì¸ì‹ ì„ê³„ê°’
            detection_cooldown: ê°™ì€ ë‹¨ì–´ ì—°ì† ì¸ì‹ ë°©ì§€ ì‹œê°„(ì´ˆ)
            device: ì¶”ë¡  ë””ë°”ì´ìŠ¤
        """
        self.logger = setup_logger("SignLanguageInference")
        
        # ì„¤ì •
        self.sequence_length = sequence_length
        self.confidence_threshold = confidence_threshold
        self.detection_cooldown = detection_cooldown
        self.device = device if device is not None else get_device()
        
        # MediaPipe ì´ˆê¸°í™”
        self.mp_extractor = MediaPipeExtractor()
        
        # ì‚¬ì „ ë¡œë”©
        self.vocab = SignVocabulary(vocab_path)
        self.vocab_size = len(self.vocab)
        
        # ëª¨ë¸ ë¡œë”©
        self.model = create_model(vocab_size=self.vocab_size, device=self.device)
        self._load_model(checkpoint_path)
        
        # ì‹œí€€ìŠ¤ ë²„í¼ (ìµœê·¼ í”„ë ˆì„ë“¤ ì €ì¥)
        self.keypoint_buffer: Deque[np.ndarray] = deque(maxlen=sequence_length)
        
        # ì¸ì‹ ê²°ê³¼ ê´€ë¦¬
        self.last_detection_time = 0
        self.last_detected_word = ""
        self.detection_history = deque(maxlen=10)  # ìµœê·¼ 10ê°œ ê²°ê³¼
        
        # í†µê³„
        self.frame_count = 0
        self.detection_count = 0
        self.fps_counter = deque(maxlen=30)
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ¯ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        self.logger.info(f"ğŸ¬ ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length} í”„ë ˆì„")
        self.logger.info(f"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {confidence_threshold}")
        self.logger.info(f"â±ï¸ ì¸ì‹ ì¿¨ë‹¤ìš´: {detection_cooldown}ì´ˆ")
        self.logger.info(f"ğŸ’¾ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info("=" * 60)
    
    def _load_model(self, checkpoint_path: Optional[Path]):
        """ëª¨ë¸ ë¡œë”©"""
        if checkpoint_path is None:
            checkpoint_path = CHECKPOINTS_PATH / "best.pt"
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        
        try:
            checkpoint = load_checkpoint(checkpoint_path, self.model.model)
            self.model.eval()
            
            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            self.logger.info(f"  - ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
            self.logger.info(f"  - ê²€ì¦ ì •í™•ë„: {checkpoint['accuracy']:.2f}%")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬"""
        self.frame_count += 1
        frame_start_time = time.time()
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints, mp_results = self.mp_extractor.extract_keypoints(frame)
        
        # ì •ê·œí™”
        normalized_keypoints = normalize_keypoints(keypoints)
        
        # ë²„í¼ì— ì¶”ê°€
        self.keypoint_buffer.append(normalized_keypoints)
        
        # ì‹œê°í™”
        annotated_frame = self.mp_extractor.draw_landmarks(frame, mp_results)
        
        # ìˆ˜ì–´ ì¸ì‹ ì‹œë„
        recognition_result = self._try_recognition()
        
        # UI ì •ë³´ ì¶”ê°€
        annotated_frame = self._add_ui_overlay(annotated_frame, recognition_result)
        
        # FPS ê³„ì‚°
        frame_time = time.time() - frame_start_time
        self.fps_counter.append(1.0 / frame_time if frame_time > 0 else 0)
        
        result_info = {
            'frame_count': self.frame_count,
            'buffer_size': len(self.keypoint_buffer),
            'fps': np.mean(self.fps_counter) if self.fps_counter else 0,
            'keypoints_detected': np.any(keypoints != 0),
            **recognition_result
        }
        
        return annotated_frame, result_info
    
    def _try_recognition(self) -> Dict:
        """ìˆ˜ì–´ ì¸ì‹ ì‹œë„"""
        current_time = time.time()
        
        # ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¼ëŠ”ì§€ í™•ì¸
        if len(self.keypoint_buffer) < self.sequence_length:
            return {
                'word_detected': False,
                'word': '',
                'confidence': 0.0,
                'top3_predictions': [],
                'status': 'collecting_frames'
            }
        
        # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸
        recent_keypoints = list(self.keypoint_buffer)[-30:]  # ìµœê·¼ 30í”„ë ˆì„
        if not any(np.any(kp != 0) for kp in recent_keypoints):
            return {
                'word_detected': False,
                'word': '',
                'confidence': 0.0,
                'top3_predictions': [],
                'status': 'no_person_detected'
            }
        
        try:
            # ì‹œí€€ìŠ¤ ì¤€ë¹„
            sequence = np.array(list(self.keypoint_buffer))
            sequence = pad_sequence(sequence, self.sequence_length)
            
            # í…ì„œ ë³€í™˜
            sequence_tensor = torch.from_numpy(sequence).unsqueeze(0).float().to(self.device)
            
            # ì¶”ë¡ 
            self.model.eval()
            with torch.no_grad():
                batch = {
                    'sequence': sequence_tensor,
                    'sequence_mask': torch.ones(1, self.sequence_length).to(self.device)
                }
                pred_outputs = self.model.predict_batch(batch)
            
            # ê²°ê³¼ ì²˜ë¦¬
            probabilities = pred_outputs['probabilities'][0].cpu().numpy()
            predicted_idx = pred_outputs['predictions'][0].item()
            max_confidence = pred_outputs['max_probabilities'][0].item()
            
            # Top-3 ì˜ˆì¸¡
            top3_indices = np.argsort(probabilities)[-3:][::-1]
            top3_predictions = [
                {
                    'word': self.vocab.index_to_word(idx),
                    'confidence': probabilities[idx] * 100,
                    'index': int(idx)
                }
                for idx in top3_indices
            ]
            
            predicted_word = self.vocab.index_to_word(predicted_idx)
            
            # ì¸ì‹ ê²°ì •
            is_confident = max_confidence >= self.confidence_threshold
            is_not_cooldown = (current_time - self.last_detection_time) >= self.detection_cooldown
            is_different_word = predicted_word != self.last_detected_word
            
            word_detected = is_confident and (is_not_cooldown or is_different_word)
            
            if word_detected:
                self.last_detection_time = current_time
                self.last_detected_word = predicted_word
                self.detection_count += 1
                
                # ì¸ì‹ ê¸°ë¡ ì €ì¥
                self.detection_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'word': predicted_word,
                    'confidence': max_confidence * 100,
                    'frame_count': self.frame_count
                })
                
                self.logger.info(f"ğŸ¯ ìˆ˜ì–´ ì¸ì‹: '{predicted_word}' (ì‹ ë¢°ë„: {max_confidence*100:.1f}%)")
            
            return {
                'word_detected': word_detected,
                'word': predicted_word if word_detected else '',
                'confidence': max_confidence * 100,
                'top3_predictions': top3_predictions,
                'status': 'recognized' if word_detected else 'analyzing',
                'raw_confidence': max_confidence * 100,
                'raw_prediction': predicted_word
            }
            
        except Exception as e:
            self.logger.error(f"ì¸ì‹ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'word_detected': False,
                'word': '',
                'confidence': 0.0,
                'top3_predictions': [],
                'status': 'error',
                'error': str(e)
            }
    
    def _add_ui_overlay(self, frame: np.ndarray, recognition_result: Dict) -> np.ndarray:
        """UI ì˜¤ë²„ë ˆì´ ì¶”ê°€"""
        height, width = frame.shape[:2]
        
        # ë°˜íˆ¬ëª… ë°°ê²½
        overlay = frame.copy()
        
        # ìƒë‹¨ ì •ë³´ íŒ¨ë„
        cv2.rectangle(overlay, (0, 0), (width, 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ì œëª©
        cv2.putText(frame, "SU:DA - Real-time Sign Language Recognition", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ìƒíƒœ ì •ë³´
        status_text = f"Buffer: {len(self.keypoint_buffer)}/{self.sequence_length} | "
        status_text += f"FPS: {np.mean(self.fps_counter):.1f} | " if self.fps_counter else "FPS: 0 | "
        status_text += f"Detections: {self.detection_count}"
        
        cv2.putText(frame, status_text, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # ì¸ì‹ ê²°ê³¼
        if recognition_result.get('word_detected', False):
            # ì¸ì‹ëœ ë‹¨ì–´ (í¬ê²Œ)
            word = recognition_result['word']
            confidence = recognition_result['confidence']
            
            # ì„±ê³µ ë°°ê²½
            cv2.rectangle(frame, (10, 70), (width-10, 110), (0, 255, 0), 2)
            cv2.putText(frame, f"Detected: {word}", (20, 95), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
            cv2.putText(frame, f"Confidence: {confidence:.1f}%", (20, 105), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
        
        elif recognition_result.get('status') == 'analyzing':
            # ë¶„ì„ ì¤‘
            raw_word = recognition_result.get('raw_prediction', '')
            raw_conf = recognition_result.get('raw_confidence', 0)
            
            cv2.rectangle(frame, (10, 70), (width-10, 110), (0, 255, 255), 1)
            cv2.putText(frame, f"Analyzing: {raw_word}", (20, 95), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
            cv2.putText(frame, f"Confidence: {raw_conf:.1f}%", (20, 105), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
        
        else:
            # ëŒ€ê¸° ì¤‘
            status_msg = {
                'collecting_frames': 'Collecting frames...',
                'no_person_detected': 'No person detected',
                'error': 'Processing error'
            }.get(recognition_result.get('status', ''), 'Ready')
            
            cv2.putText(frame, f"Status: {status_msg}", (20, 95), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ìš°ì¸¡ Top-3 ì˜ˆì¸¡ (ì‘ê²Œ)
        if recognition_result.get('top3_predictions'):
            y_offset = 140
            cv2.putText(frame, "Top-3 Predictions:", (width-250, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            for i, pred in enumerate(recognition_result['top3_predictions'][:3]):
                y_pos = y_offset + 25 + (i * 20)
                text = f"{i+1}. {pred['word']} ({pred['confidence']:.1f}%)"
                color = (0, 255, 0) if i == 0 else (200, 200, 200)
                cv2.putText(frame, text, (width-240, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        # ìµœê·¼ ì¸ì‹ ê¸°ë¡ (í•˜ë‹¨)
        if self.detection_history:
            cv2.putText(frame, "Recent detections:", (10, height-60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            recent = list(self.detection_history)[-3:]  # ìµœê·¼ 3ê°œ
            for i, detection in enumerate(recent):
                y_pos = height - 40 + (i * 15)
                time_str = detection['timestamp'][-8:-3]  # HH:MMë§Œ
                text = f"{time_str} - {detection['word']} ({detection['confidence']:.1f}%)"
                cv2.putText(frame, text, (10, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
        
        # ì¡°ì‘ ì•ˆë‚´
        cv2.putText(frame, "Press 'q' to quit, 'r' to reset, 's' to save", 
                   (10, height-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return frame
    
    def reset_buffer(self):
        """ë²„í¼ ì´ˆê¸°í™”"""
        self.keypoint_buffer.clear()
        self.last_detection_time = 0
        self.last_detected_word = ""
        self.logger.info("ğŸ”„ ë²„í¼ ì´ˆê¸°í™”")
    
    def save_detection_log(self):
        """ì¸ì‹ ê¸°ë¡ ì €ì¥"""
        if not self.detection_history:
            self.logger.info("ì €ì¥í•  ì¸ì‹ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = Path(f"sign_detection_log_{timestamp}.txt")
        
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"SU:DA ìˆ˜ì–´ ì¸ì‹ ë¡œê·¸ - {datetime.now()}\n")
            f.write("=" * 50 + "\n")
            f.write(f"ì´ í”„ë ˆì„ ìˆ˜: {self.frame_count}\n")
            f.write(f"ì´ ì¸ì‹ ìˆ˜: {self.detection_count}\n")
            f.write(f"ì¸ì‹ ì„±ê³µë¥ : {self.detection_count/self.frame_count*100:.2f}%\n\n")
            
            f.write("ì¸ì‹ ê¸°ë¡:\n")
            for detection in self.detection_history:
                f.write(f"{detection['timestamp']} - {detection['word']} "
                       f"({detection['confidence']:.1f}%) [Frame: {detection['frame_count']}]\n")
        
        self.logger.info(f"ğŸ“ ì¸ì‹ ë¡œê·¸ ì €ì¥: {log_file}")
    
    def run_webcam_inference(self, camera_id: int = 0, window_size: Tuple[int, int] = (1280, 720)):
        """ì›¹ìº  ì‹¤ì‹œê°„ ì¶”ë¡  ì‹¤í–‰"""
        self.logger.info(f"ğŸ¥ ì›¹ìº  ì¶”ë¡  ì‹œì‘ (ì¹´ë©”ë¼ ID: {camera_id})")
        
        # ì›¹ìº  ì´ˆê¸°í™”
        cap = cv2.VideoCapture(camera_id)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, window_size[0])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, window_size[1])
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        if not cap.isOpened():
            raise RuntimeError(f"ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ID: {camera_id})")
        
        self.logger.info("ì›¹ìº  ì—°ê²° ì„±ê³µ! ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        self.logger.info("ì¡°ì‘ë²•: 'q'=ì¢…ë£Œ, 'r'=ë²„í¼ì´ˆê¸°í™”, 's'=ë¡œê·¸ì €ì¥")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    self.logger.error("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ì¢Œìš° ë°˜ì „ (ê±°ìš¸ íš¨ê³¼)
                frame = cv2.flip(frame, 1)
                
                # í”„ë ˆì„ ì²˜ë¦¬
                processed_frame, result_info = self.process_frame(frame)
                
                # í™”ë©´ ì¶œë ¥
                cv2.imshow('SU:DA - Sign Language Recognition', processed_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q'):
                    self.logger.info("ì‚¬ìš©ì ì¢…ë£Œ ìš”ì²­")
                    break
                elif key == ord('r'):
                    self.reset_buffer()
                elif key == ord('s'):
                    self.save_detection_log()
                elif key == ord(' '):  # ìŠ¤í˜ì´ìŠ¤ë°”ë¡œ ì¼ì‹œì •ì§€
                    cv2.waitKey(0)
        
        except KeyboardInterrupt:
            self.logger.info("í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ë¡œ ì¢…ë£Œ")
        
        except Exception as e:
            self.logger.error(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            # ì •ë¦¬
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„
            self.logger.info("=" * 60)
            self.logger.info("ğŸ‰ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì¢…ë£Œ")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬ í”„ë ˆì„: {self.frame_count:,}")
            self.logger.info(f"ğŸ¯ ì´ ì¸ì‹ íšŸìˆ˜: {self.detection_count}")
            if self.frame_count > 0:
                self.logger.info(f"ğŸ“ˆ ì¸ì‹ ë¹„ìœ¨: {self.detection_count/self.frame_count*100:.2f}%")
            if self.fps_counter:
                self.logger.info(f"âš¡ í‰ê·  FPS: {np.mean(self.fps_counter):.1f}")
            self.logger.info("=" * 60)
            
            # ìë™ ë¡œê·¸ ì €ì¥
            if self.detection_history:
                self.save_detection_log()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        inference_system = SignLanguageInference(
            checkpoint_path=CHECKPOINTS_PATH / "best.pt",
            sequence_length=60,  # 2ì´ˆ @ 30fps
            confidence_threshold=0.7,
            detection_cooldown=1.5
        )
        
        # ì›¹ìº  ì¶”ë¡  ì‹¤í–‰
        inference_system.run_webcam_inference(
            camera_id=0,
            window_size=(1280, 720)
        )
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("âœ… ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#!/usr/bin/env python3
"""
SU:DA - ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í‰ê°€
í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ë° ë¶„ì„
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, 
    confusion_matrix, classification_report
)
from collections import defaultdict, Counter
import json
from tqdm import tqdm
import time

from utils import (
    setup_logger, load_checkpoint, get_device,
    CHECKPOINTS_PATH, LOGS_PATH, format_time
)
from model import create_model
from dataset import create_data_loaders
from vocab import SignVocabulary

class SignLanguageEvaluator:
    """ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self,
                 checkpoint_path: Optional[Path] = None,
                 vocab_path: Optional[Path] = None,
                 batch_size: int = 32,
                 device: Optional[torch.device] = None,
                 save_predictions: bool = True):
        """
        Args:
            checkpoint_path: í‰ê°€í•  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            vocab_path: ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            device: í‰ê°€ ë””ë°”ì´ìŠ¤
            save_predictions: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        """
        self.logger = setup_logger("SignLanguageEvaluator", "eval.log")
        self.save_predictions = save_predictions
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = device if device is not None else get_device()
        
        # ì‚¬ì „ ë¡œë”©
        self.vocab = SignVocabulary(vocab_path)
        self.vocab_size = len(self.vocab)
        
        # ëª¨ë¸ ìƒì„± ë° ë¡œë”©
        self.model = create_model(vocab_size=self.vocab_size, device=self.device)
        
        if checkpoint_path is None:
            checkpoint_path = CHECKPOINTS_PATH / "best.pt"
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        
        self._load_model(checkpoint_path)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        self.train_loader, self.val_loader = create_data_loaders(
            train_batch_size=batch_size,
            val_batch_size=batch_size,
            vocab_path=vocab_path
        )
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥ìš©
        self.results = {}
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ” ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í‰ê°€ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        self.logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_loader.dataset):,}ê°œ")
        self.logger.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(self.val_loader.dataset):,}ê°œ")
        self.logger.info(f"ğŸ’¾ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
        self.logger.info("=" * 60)
    
    def _load_model(self, checkpoint_path: Path):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            checkpoint = load_checkpoint(checkpoint_path, self.model.model)
            self.model.eval()
            
            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            self.logger.info(f"  - ì—í¬í¬: {checkpoint['epoch']}")
            self.logger.info(f"  - ê²€ì¦ ì†ì‹¤: {checkpoint['loss']:.4f}")
            self.logger.info(f"  - ê²€ì¦ ì •í™•ë„: {checkpoint['accuracy']:.2f}%")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def evaluate_dataset(self, data_loader, dataset_name: str) -> Dict:
        """ë°ì´í„°ì…‹ í‰ê°€"""
        self.logger.info(f"ğŸ¯ {dataset_name} ë°ì´í„°ì…‹ í‰ê°€ ì‹œì‘")
        
        self.model.eval()
        start_time = time.time()
        
        all_predictions = []
        all_labels = []
        all_probabilities = []
        all_word_labels = []
        all_word_ids = []
        
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            pbar = tqdm(data_loader, desc=f"{dataset_name} í‰ê°€ ì¤‘")
            
            for batch in pbar:
                # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(self.device)
                
                # ì˜ˆì¸¡
                outputs = self.model(batch)
                pred_outputs = self.model.predict_batch(batch)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                loss = outputs['loss']
                total_loss += loss.item()
                num_batches += 1
                
                predictions = pred_outputs['predictions'].cpu().numpy()
                probabilities = pred_outputs['probabilities'].cpu().numpy()
                labels = batch['label'].cpu().numpy()
                word_labels = batch['word_label']
                word_ids = batch['word_id'].cpu().numpy()
                
                all_predictions.extend(predictions)
                all_labels.extend(labels)
                all_probabilities.extend(probabilities)
                all_word_labels.extend(word_labels)
                all_word_ids.extend(word_ids)
                
                # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
                accuracy = accuracy_score(labels, predictions) * 100
                pbar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Acc': f"{accuracy:.2f}%"
                })
        
        eval_time = time.time() - start_time
        avg_loss = total_loss / num_batches
        
        # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._compute_detailed_metrics(
            all_predictions, all_labels, all_probabilities,
            all_word_labels, all_word_ids
        )
        metrics['loss'] = avg_loss
        metrics['eval_time'] = eval_time
        
        self.logger.info(f"âœ… {dataset_name} í‰ê°€ ì™„ë£Œ ({format_time(eval_time)})")
        self.logger.info(f"  - ì „ì²´ ì •í™•ë„: {metrics['accuracy']:.2f}%")
        self.logger.info(f"  - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        self.logger.info(f"  - Precision: {metrics['macro_precision']:.2f}%")
        self.logger.info(f"  - Recall: {metrics['macro_recall']:.2f}%")
        self.logger.info(f"  - F1-Score: {metrics['macro_f1']:.2f}%")
        
        return metrics
    
    def _compute_detailed_metrics(self, predictions: List[int], labels: List[int],
                                probabilities: np.ndarray, word_labels: List[str],
                                word_ids: List[int]) -> Dict:
        """ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        accuracy = accuracy_score(labels, predictions) * 100
        
        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        precision, recall, f1, support = precision_recall_fscore_support(
            labels, predictions, average=None, zero_division=0
        )
        
        # ë§¤í¬ë¡œ í‰ê· 
        macro_precision = np.mean(precision) * 100
        macro_recall = np.mean(recall) * 100
        macro_f1 = np.mean(f1) * 100
        
        # ë§ˆì´í¬ë¡œ í‰ê·  (ì „ì²´ ì •í™•ë„ì™€ ë™ì¼)
        micro_precision = accuracy
        micro_recall = accuracy
        micro_f1 = accuracy
        
        # ê°€ì¤‘ í‰ê· 
        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
            labels, predictions, average='weighted', zero_division=0
        )
        weighted_precision *= 100
        weighted_recall *= 100
        weighted_f1 *= 100
        
        # Top-k ì •í™•ë„
        top3_accuracy = self._compute_topk_accuracy(probabilities, labels, k=3)
        top5_accuracy = self._compute_topk_accuracy(probabilities, labels, k=5)
        
        # ì‹ ë¢°ë„ í†µê³„
        max_probs = np.max(probabilities, axis=1)
        confidence_stats = {
            'mean_confidence': np.mean(max_probs) * 100,
            'median_confidence': np.median(max_probs) * 100,
            'std_confidence': np.std(max_probs) * 100,
            'min_confidence': np.min(max_probs) * 100,
            'max_confidence': np.max(max_probs) * 100
        }
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(labels, predictions)
        
        # ë‹¨ì–´ë³„ ì„±ëŠ¥ ë¶„ì„
        word_performance = self._analyze_word_performance(
            predictions, labels, word_labels, word_ids, probabilities
        )
        
        return {
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            'accuracy': accuracy,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'micro_precision': micro_precision,
            'micro_recall': micro_recall,
            'micro_f1': micro_f1,
            'weighted_precision': weighted_precision,
            'weighted_recall': weighted_recall,
            'weighted_f1': weighted_f1,
            
            # Top-k ì •í™•ë„
            'top3_accuracy': top3_accuracy,
            'top5_accuracy': top5_accuracy,
            
            # ì‹ ë¢°ë„ í†µê³„
            **confidence_stats,
            
            # ìƒì„¸ ë°ì´í„°
            'class_precision': precision,
            'class_recall': recall,
            'class_f1': f1,
            'class_support': support,
            'confusion_matrix': cm,
            'word_performance': word_performance,
            
            # ì›ë³¸ ë°ì´í„° (ì €ì¥ìš©)
            'predictions': predictions,
            'labels': labels,
            'probabilities': probabilities,
            'word_labels': word_labels,
            'word_ids': word_ids
        }
    
    def _compute_topk_accuracy(self, probabilities: np.ndarray, labels: List[int], k: int) -> float:
        """Top-k ì •í™•ë„ ê³„ì‚°"""
        top_k_indices = np.argsort(probabilities, axis=1)[:, -k:]
        correct = 0
        
        for i, true_label in enumerate(labels):
            if true_label in top_k_indices[i]:
                correct += 1
        
        return (correct / len(labels)) * 100
    
    def _analyze_word_performance(self, predictions: List[int], labels: List[int],
                                word_labels: List[str], word_ids: List[int],
                                probabilities: np.ndarray) -> Dict:
        """ë‹¨ì–´ë³„ ì„±ëŠ¥ ë¶„ì„"""
        
        word_stats = defaultdict(lambda: {
            'correct': 0, 'total': 0, 'confidences': []
        })
        
        for pred, label, word_label, word_id, prob in zip(
            predictions, labels, word_labels, word_ids, probabilities
        ):
            word_stats[word_label]['total'] += 1
            word_stats[word_label]['confidences'].append(np.max(prob))
            
            if pred == label:
                word_stats[word_label]['correct'] += 1
        
        # ë‹¨ì–´ë³„ ì •í™•ë„ ê³„ì‚°
        word_accuracies = {}
        for word, stats in word_stats.items():
            accuracy = (stats['correct'] / stats['total']) * 100
            avg_confidence = np.mean(stats['confidences']) * 100
            
            word_accuracies[word] = {
                'accuracy': accuracy,
                'correct': stats['correct'],
                'total': stats['total'],
                'avg_confidence': avg_confidence
            }
        
        # ì„±ëŠ¥ë³„ ë‹¨ì–´ ë¶„ë¥˜
        best_words = sorted(word_accuracies.items(), 
                           key=lambda x: x[1]['accuracy'], reverse=True)[:10]
        worst_words = sorted(word_accuracies.items(), 
                            key=lambda x: x[1]['accuracy'])[:10]
        
        return {
            'word_accuracies': word_accuracies,
            'best_words': best_words,
            'worst_words': worst_words,
            'avg_word_accuracy': np.mean([w['accuracy'] for w in word_accuracies.values()])
        }
    
    def save_detailed_results(self, train_metrics: Dict, val_metrics: Dict):
        """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
        if not self.save_predictions:
            return
        
        results_dir = LOGS_PATH / "evaluation_results"
        results_dir.mkdir(exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # 1. ì „ì²´ ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary = {
            'timestamp': timestamp,
            'train_metrics': {k: v for k, v in train_metrics.items() 
                            if not isinstance(v, (np.ndarray, list))},
            'val_metrics': {k: v for k, v in val_metrics.items() 
                          if not isinstance(v, (np.ndarray, list))},
            'vocab_size': self.vocab_size
        }
        
        summary_file = results_dir / f"evaluation_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # 2. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        self._save_predictions_csv(val_metrics, results_dir, timestamp)
        
        # 3. ì‹œê°í™” ì €ì¥
        self._save_visualizations(train_metrics, val_metrics, results_dir, timestamp)
        
        # 4. ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì €ì¥
        self._save_classification_report(val_metrics, results_dir, timestamp)
        
        self.logger.info(f"ğŸ“Š í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir}")
    
    def _save_predictions_csv(self, metrics: Dict, results_dir: Path, timestamp: str):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        
        predictions_data = []
        for i, (pred, label, word_label, word_id, prob) in enumerate(zip(
            metrics['predictions'], metrics['labels'], 
            metrics['word_labels'], metrics['word_ids'], metrics['probabilities']
        )):
            
            pred_word = self.vocab.index_to_word(pred)
            true_word = self.vocab.index_to_word(label)
            max_prob = np.max(prob)
            
            predictions_data.append({
                'sample_id': i,
                'word_id': word_id,
                'true_label_idx': label,
                'pred_label_idx': pred,
                'true_word': true_word,
                'pred_word': pred_word,
                'word_label': word_label,
                'max_probability': max_prob,
                'is_correct': pred == label
            })
        
        df = pd.DataFrame(predictions_data)
        predictions_file = results_dir / f"predictions_{timestamp}.csv"
        df.to_csv(predictions_file, index=False, encoding='utf-8')
        
        self.logger.info(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {predictions_file}")
    
    def _save_visualizations(self, train_metrics: Dict, val_metrics: Dict, 
                           results_dir: Path, timestamp: str):
        """ì‹œê°í™” ì €ì¥"""
        
        # 1. í˜¼ë™ í–‰ë ¬ (ìƒìœ„ 20ê°œ í´ë˜ìŠ¤ë§Œ)
        self._plot_confusion_matrix(val_metrics['confusion_matrix'], 
                                   results_dir, timestamp)
        
        # 2. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        self._plot_class_performance(val_metrics, results_dir, timestamp)
        
        # 3. ì‹ ë¢°ë„ ë¶„í¬
        self._plot_confidence_distribution(val_metrics, results_dir, timestamp)
        
        # 4. ë‹¨ì–´ë³„ ì„±ëŠ¥
        self._plot_word_performance(val_metrics, results_dir, timestamp)
    
    def _plot_confusion_matrix(self, cm: np.ndarray, results_dir: Path, timestamp: str):
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
        
        # ìƒìœ„ 20ê°œ í´ë˜ìŠ¤ë§Œ ì„ íƒ (ê°€ì¥ ë§ì´ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ë“¤)
        top_classes = np.argsort(np.sum(cm, axis=1))[-20:]
        cm_subset = cm[np.ix_(top_classes, top_classes)]
        
        # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        class_names = [self.vocab.index_to_word(i) for i in top_classes]
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('í˜¼ë™ í–‰ë ¬ (ìƒìœ„ 20ê°œ í´ë˜ìŠ¤)', fontsize=14, fontweight='bold')
        plt.xlabel('ì˜ˆì¸¡ ë¼ë²¨')
        plt.ylabel('ì‹¤ì œ ë¼ë²¨')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        cm_file = results_dir / f"confusion_matrix_{timestamp}.png"
        plt.savefig(cm_file, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_class_performance(self, metrics: Dict, results_dir: Path, timestamp: str):
        """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì‹œê°í™”"""
        
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        # Precision, Recall, F1 ë¶„í¬
        ax1.hist(metrics['class_precision'], bins=30, alpha=0.7, label='Precision')
        ax1.hist(metrics['class_recall'], bins=30, alpha=0.7, label='Recall')
        ax1.hist(metrics['class_f1'], bins=30, alpha=0.7, label='F1-Score')
        ax1.set_xlabel('ì ìˆ˜')
        ax1.set_ylabel('í´ë˜ìŠ¤ ìˆ˜')
        ax1.set_title('í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„í¬')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Support vs Performance
        ax2.scatter(metrics['class_support'], metrics['class_f1'], alpha=0.6)
        ax2.set_xlabel('Support (ìƒ˜í”Œ ìˆ˜)')
        ax2.set_ylabel('F1-Score')
        ax2.set_title('ìƒ˜í”Œ ìˆ˜ vs F1-Score')
        ax2.grid(True, alpha=0.3)
        
        # Top/Bottom ì„±ëŠ¥ í´ë˜ìŠ¤ë“¤
        sorted_f1 = sorted(enumerate(metrics['class_f1']), key=lambda x: x[1])
        bottom_10 = sorted_f1[:10]
        top_10 = sorted_f1[-10:]
        
        bottom_names = [self.vocab.index_to_word(i) for i, _ in bottom_10]
        bottom_scores = [score for _, score in bottom_10]
        top_names = [self.vocab.index_to_word(i) for i, _ in top_10]
        top_scores = [score for _, score in top_10]
        
        y_pos = np.arange(10)
        ax3.barh(y_pos, bottom_scores, alpha=0.7, color='red', label='Bottom 10')
        ax3.barh(y_pos + 11, top_scores, alpha=0.7, color='green', label='Top 10')
        ax3.set_yticks(list(y_pos) + list(y_pos + 11))
        ax3.set_yticklabels(bottom_names + top_names, fontsize=8)
        ax3.set_xlabel('F1-Score')
        ax3.set_title('ìµœê³ /ìµœì•… ì„±ëŠ¥ í´ë˜ìŠ¤')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        performance_file = results_dir / f"class_performance_{timestamp}.png"
        plt.savefig(performance_file, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_confidence_distribution(self, metrics: Dict, results_dir: Path, timestamp: str):
        """ì‹ ë¢°ë„ ë¶„í¬ ì‹œê°í™”"""
        
        max_probs = np.max(metrics['probabilities'], axis=1)
        correct_mask = np.array(metrics['predictions']) == np.array(metrics['labels'])
        
        plt.figure(figsize=(12, 8))
        
        # ì •ë‹µ/ì˜¤ë‹µë³„ ì‹ ë¢°ë„ ë¶„í¬
        plt.subplot(2, 2, 1)
        plt.hist(max_probs[correct_mask], bins=30, alpha=0.7, label='ì •ë‹µ', color='green')
        plt.hist(max_probs[~correct_mask], bins=30, alpha=0.7, label='ì˜¤ë‹µ', color='red')
        plt.xlabel('ìµœëŒ€ í™•ë¥ ')
        plt.ylabel('ë¹ˆë„')
        plt.title('ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ì‹ ë¢°ë„ vs ì •í™•ë„
        plt.subplot(2, 2, 2)
        confidence_bins = np.linspace(0, 1, 11)
        accuracies = []
        
        for i in range(len(confidence_bins) - 1):
            mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])
            if mask.sum() > 0:
                accuracy = correct_mask[mask].mean()
                accuracies.append(accuracy)
            else:
                accuracies.append(0)
        
        bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
        plt.plot(bin_centers, accuracies, 'bo-', linewidth=2, markersize=6)
        plt.xlabel('ì‹ ë¢°ë„ êµ¬ê°„')
        plt.ylabel('ì •í™•ë„')
        plt.title('ì‹ ë¢°ë„ë³„ ì •í™•ë„')
        plt.grid(True, alpha=0.3)
        
        # ë³´ì • ê³¡ì„  (Calibration Curve)
        plt.subplot(2, 2, 3)
        from sklearn.calibration import calibration_curve
        fraction_positives, mean_predicted_value = calibration_curve(
            correct_mask, max_probs, n_bins=10
        )
        plt.plot(mean_predicted_value, fraction_positives, 'bo-', label='ëª¨ë¸')
        plt.plot([0, 1], [0, 1], 'k--', label='ì™„ë²½í•œ ë³´ì •')
        plt.xlabel('í‰ê·  ì˜ˆì¸¡ í™•ë¥ ')
        plt.ylabel('ì‹¤ì œ ì •ë‹µ ë¹„ìœ¨')
        plt.title('ë³´ì • ê³¡ì„ ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ì‹ ë¢°ë„ í†µê³„
        plt.subplot(2, 2, 4)
        stats_text = f"""
        í‰ê·  ì‹ ë¢°ë„: {metrics['mean_confidence']:.2f}%
        ì¤‘ì•™ê°’ ì‹ ë¢°ë„: {metrics['median_confidence']:.2f}%
        í‘œì¤€í¸ì°¨: {metrics['std_confidence']:.2f}%
        ìµœì†Œê°’: {metrics['min_confidence']:.2f}%
        ìµœëŒ€ê°’: {metrics['max_confidence']:.2f}%
        
        ì •ë‹µ í‰ê·  ì‹ ë¢°ë„: {max_probs[correct_mask].mean()*100:.2f}%
        ì˜¤ë‹µ í‰ê·  ì‹ ë¢°ë„: {max_probs[~correct_mask].mean()*100:.2f}%
        """
        plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')
        plt.axis('off')
        plt.title('ì‹ ë¢°ë„ í†µê³„')
        
        plt.tight_layout()
        confidence_file = results_dir / f"confidence_analysis_{timestamp}.png"
        plt.savefig(confidence_file, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_word_performance(self, metrics: Dict, results_dir: Path, timestamp: str):
        """ë‹¨ì–´ë³„ ì„±ëŠ¥ ì‹œê°í™”"""
        
        word_perf = metrics['word_performance']
        
        plt.figure(figsize=(15, 10))
        
        # ìµœê³  ì„±ëŠ¥ ë‹¨ì–´ë“¤
        plt.subplot(2, 2, 1)
        best_words = word_perf['best_words'][:10]
        words = [w[0] for w in best_words]
        accs = [w[1]['accuracy'] for w in best_words]
        
        plt.barh(range(len(words)), accs, color='green', alpha=0.7)
        plt.yticks(range(len(words)), words)
        plt.xlabel('ì •í™•ë„ (%)')
        plt.title('ìµœê³  ì„±ëŠ¥ ë‹¨ì–´ (Top 10)')
        plt.grid(True, alpha=0.3)
        
        # ìµœì•… ì„±ëŠ¥ ë‹¨ì–´ë“¤
        plt.subplot(2, 2, 2)
        worst_words = word_perf['worst_words'][:10]
        words = [w[0] for w in worst_words]
        accs = [w[1]['accuracy'] for w in worst_words]
        
        plt.barh(range(len(words)), accs, color='red', alpha=0.7)
        plt.yticks(range(len(words)), words)
        plt.xlabel('ì •í™•ë„ (%)')
        plt.title('ìµœì•… ì„±ëŠ¥ ë‹¨ì–´ (Bottom 10)')
        plt.grid(True, alpha=0.3)
        
        # ë‹¨ì–´ë³„ ì •í™•ë„ ë¶„í¬
        plt.subplot(2, 2, 3)
        all_accs = [w['accuracy'] for w in word_perf['word_accuracies'].values()]
        plt.hist(all_accs, bins=30, alpha=0.7, color='blue')
        plt.axvline(word_perf['avg_word_accuracy'], color='red', linestyle='--', 
                   label=f'í‰ê· : {word_perf["avg_word_accuracy"]:.1f}%')
        plt.xlabel('ì •í™•ë„ (%)')
        plt.ylabel('ë‹¨ì–´ ìˆ˜')
        plt.title('ë‹¨ì–´ë³„ ì •í™•ë„ ë¶„í¬')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ìƒ˜í”Œ ìˆ˜ vs ì •í™•ë„
        plt.subplot(2, 2, 4)
        totals = [w['total'] for w in word_perf['word_accuracies'].values()]
        accs = [w['accuracy'] for w in word_perf['word_accuracies'].values()]
        
        plt.scatter(totals, accs, alpha=0.6)
        plt.xlabel('ìƒ˜í”Œ ìˆ˜')
        plt.ylabel('ì •í™•ë„ (%)')
        plt.title('ìƒ˜í”Œ ìˆ˜ vs ì •í™•ë„')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        word_perf_file = results_dir / f"word_performance_{timestamp}.png"
        plt.savefig(word_perf_file, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _save_classification_report(self, metrics: Dict, results_dir: Path, timestamp: str):
        """ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì €ì¥"""
        
        # scikit-learn ë¶„ë¥˜ ë¦¬í¬íŠ¸
        target_names = [self.vocab.index_to_word(i) for i in range(self.vocab_size)]
        report = classification_report(
            metrics['labels'], metrics['predictions'],
            target_names=target_names, output_dict=True, zero_division=0
        )
        
        # JSONìœ¼ë¡œ ì €ì¥
        report_file = results_dir / f"classification_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # í…ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ì €ì¥
        text_report = classification_report(
            metrics['labels'], metrics['predictions'],
            target_names=target_names, zero_division=0
        )
        
        text_file = results_dir / f"classification_report_{timestamp}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        self.logger.info(f"ğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    
    def run_full_evaluation(self) -> Dict:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ì „ì²´ í‰ê°€ ì‹œì‘!")
        
        start_time = time.time()
        
        # í•™ìŠµ ë°ì´í„° í‰ê°€
        train_metrics = self.evaluate_dataset(self.train_loader, "í•™ìŠµ")
        
        # ê²€ì¦ ë°ì´í„° í‰ê°€
        val_metrics = self.evaluate_dataset(self.val_loader, "ê²€ì¦")
        
        # ê²°ê³¼ ì €ì¥
        self.save_detailed_results(train_metrics, val_metrics)
        
        # ìš”ì•½ ì¶œë ¥
        total_time = time.time() - start_time
        
        self.logger.info("=" * 80)
        self.logger.info("ğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ!")
        self.logger.info("=" * 80)
        self.logger.info(f"â±ï¸  ì´ í‰ê°€ ì‹œê°„: {format_time(total_time)}")
        self.logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ì •í™•ë„: {train_metrics['accuracy']:.2f}%")
        self.logger.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„° ì •í™•ë„: {val_metrics['accuracy']:.2f}%")
        self.logger.info(f"ğŸ¯ Top-3 ì •í™•ë„: {val_metrics['top3_accuracy']:.2f}%")
        self.logger.info(f"ğŸ¯ Top-5 ì •í™•ë„: {val_metrics['top5_accuracy']:.2f}%")
        self.logger.info(f"ğŸ“ˆ Macro F1-Score: {val_metrics['macro_f1']:.2f}%")
        self.logger.info(f"ğŸ“ˆ Weighted F1-Score: {val_metrics['weighted_f1']:.2f}%")
        self.logger.info(f"ğŸ”’ í‰ê·  ì‹ ë¢°ë„: {val_metrics['mean_confidence']:.2f}%")
        self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {LOGS_PATH / 'evaluation_results'}")
        self.logger.info("=" * 80)
        
        return {
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'evaluation_time': total_time
        }
    
    def quick_evaluation(self, num_samples: int = 100) -> Dict:
        """ë¹ ë¥¸ í‰ê°€ (ì¼ë¶€ ìƒ˜í”Œë§Œ)"""
        self.logger.info(f"âš¡ ë¹ ë¥¸ í‰ê°€ ì‹œì‘ ({num_samples}ê°œ ìƒ˜í”Œ)")
        
        self.model.eval()
        
        # ê²€ì¦ ë°ì´í„°ì—ì„œ ì¼ë¶€ë§Œ ì¶”ì¶œ
        sample_indices = np.random.choice(
            len(self.val_loader.dataset), 
            min(num_samples, len(self.val_loader.dataset)), 
            replace=False
        )
        
        predictions = []
        labels = []
        confidences = []
        
        with torch.no_grad():
            for idx in tqdm(sample_indices, desc="ë¹ ë¥¸ í‰ê°€"):
                sample = self.val_loader.dataset[idx]
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                batch = {key: tensor.unsqueeze(0).to(self.device) 
                        for key, tensor in sample.items() 
                        if isinstance(tensor, torch.Tensor)}
                batch['word_label'] = [sample['word_label']]
                
                # ì˜ˆì¸¡
                pred_outputs = self.model.predict_batch(batch)
                
                predictions.append(pred_outputs['predictions'][0].item())
                labels.append(sample['label'].item())
                confidences.append(pred_outputs['max_probabilities'][0].item())
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(labels, predictions) * 100
        avg_confidence = np.mean(confidences) * 100
        
        correct_mask = np.array(predictions) == np.array(labels)
        correct_confidence = np.mean(np.array(confidences)[correct_mask]) * 100
        wrong_confidence = np.mean(np.array(confidences)[~correct_mask]) * 100
        
        self.logger.info(f"âœ… ë¹ ë¥¸ í‰ê°€ ì™„ë£Œ:")
        self.logger.info(f"  - ì •í™•ë„: {accuracy:.2f}%")
        self.logger.info(f"  - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}%")
        self.logger.info(f"  - ì •ë‹µ ì‹ ë¢°ë„: {correct_confidence:.2f}%")
        self.logger.info(f"  - ì˜¤ë‹µ ì‹ ë¢°ë„: {wrong_confidence:.2f}%")
        
        return {
            'accuracy': accuracy,
            'avg_confidence': avg_confidence,
            'correct_confidence': correct_confidence,
            'wrong_confidence': wrong_confidence,
            'num_samples': len(sample_indices)
        }
    
    def evaluate_single_sample(self, sample_idx: int) -> Dict:
        """ë‹¨ì¼ ìƒ˜í”Œ ìƒì„¸ ë¶„ì„"""
        if sample_idx >= len(self.val_loader.dataset):
            raise IndexError(f"ìƒ˜í”Œ ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼: {sample_idx}")
        
        sample = self.val_loader.dataset[sample_idx]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        batch = {key: tensor.unsqueeze(0).to(self.device) 
                for key, tensor in sample.items() 
                if isinstance(tensor, torch.Tensor)}
        batch['word_label'] = [sample['word_label']]
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(batch)
            pred_outputs = self.model.predict_batch(batch)
        
        # ìƒìœ„ 5ê°œ ì˜ˆì¸¡ ê²°ê³¼
        probs = pred_outputs['probabilities'][0].cpu().numpy()
        top5_indices = np.argsort(probs)[-5:][::-1]
        top5_words = [self.vocab.index_to_word(idx) for idx in top5_indices]
        top5_probs = [probs[idx] * 100 for idx in top5_indices]
        
        result = {
            'sample_info': {
                'index': sample_idx,
                'word_id': sample['word_id'].item(),
                'true_word': sample['word_label'],
                'true_label_idx': sample['label'].item(),
                'original_length': sample['original_length'].item()
            },
            'prediction': {
                'predicted_idx': pred_outputs['predictions'][0].item(),
                'predicted_word': self.vocab.index_to_word(pred_outputs['predictions'][0].item()),
                'confidence': pred_outputs['max_probabilities'][0].item() * 100,
                'is_correct': pred_outputs['predictions'][0].item() == sample['label'].item()
            },
            'top5_predictions': {
                'words': top5_words,
                'probabilities': top5_probs,
                'indices': top5_indices.tolist()
            },
            'loss': outputs['loss'].item()
        }
        
        self.logger.info(f"ğŸ” ìƒ˜í”Œ {sample_idx} ë¶„ì„:")
        self.logger.info(f"  ì‹¤ì œ: {result['sample_info']['true_word']}")
        self.logger.info(f"  ì˜ˆì¸¡: {result['prediction']['predicted_word']}")
        self.logger.info(f"  ì‹ ë¢°ë„: {result['prediction']['confidence']:.2f}%")
        self.logger.info(f"  ì •ë‹µ ì—¬ë¶€: {'âœ…' if result['prediction']['is_correct'] else 'âŒ'}")
        self.logger.info(f"  Top-5: {', '.join([f'{w}({p:.1f}%)' for w, p in zip(top5_words, top5_probs)])}")
        
        return result

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ê¸°ë³¸ í‰ê°€
    evaluator = SignLanguageEvaluator(
        checkpoint_path=CHECKPOINTS_PATH / "best.pt",
        batch_size=32,
        save_predictions=True
    )
    
    # ì „ì²´ í‰ê°€ ì‹¤í–‰
    results = evaluator.run_full_evaluation()
    
    # ë¹ ë¥¸ í‰ê°€ë„ ì‹¤í–‰
    quick_results = evaluator.quick_evaluation(num_samples=200)
    
    # ëª‡ ê°œ ìƒ˜í”Œ ìƒì„¸ ë¶„ì„
    print("\nğŸ” ìƒ˜í”Œë³„ ìƒì„¸ ë¶„ì„:")
    for i in range(min(5, len(evaluator.val_loader.dataset))):
        evaluator.evaluate_single_sample(i)
        print("-" * 40)
    
    print("\nğŸ‰ í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("âœ… í‰ê°€ ì™„ë£Œ!")
    else:
        print("âŒ í‰ê°€ ì‹¤íŒ¨!")
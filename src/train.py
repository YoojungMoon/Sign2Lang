#!/usr/bin/env python3
"""
SU:DA - ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ
BiLSTM ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

from utils import (
    setup_logger, save_checkpoint, load_checkpoint, get_device,
    LEARNING_RATE, BATCH_SIZE, NUM_EPOCHS, CHECKPOINTS_PATH, LOGS_PATH,
    format_time, count_parameters
)
from model import create_model
from dataset import create_data_loaders
from vocab import SignVocabulary

class SignLanguageTrainer:
    """ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self,
                 vocab_path: Optional[Path] = None,
                 learning_rate: float = LEARNING_RATE,
                 batch_size: int = BATCH_SIZE,
                 num_epochs: int = NUM_EPOCHS,
                 device: Optional[torch.device] = None,
                 resume_from: Optional[Path] = None,
                 use_class_weights: bool = True,
                 gradient_clip_val: float = 1.0,
                 save_every: int = 5):
        """
        Args:
            vocab_path: ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            learning_rate: í•™ìŠµë¥ 
            batch_size: ë°°ì¹˜ í¬ê¸°
            num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤
            resume_from: ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            use_class_weights: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
            gradient_clip_val: ê¸°ìš¸ê¸° í´ë¦¬í•‘ ê°’
            save_every: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°
        """
        self.logger = setup_logger("SignLanguageTrainer", "train.log")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.gradient_clip_val = gradient_clip_val
        self.save_every = save_every
        self.use_class_weights = use_class_weights
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = device if device is not None else get_device()
        
        # ì‚¬ì „ ë¡œë”©
        self.vocab = SignVocabulary(vocab_path)
        self.vocab_size = len(self.vocab)
        
        # ëª¨ë¸ ìƒì„±
        self.model = create_model(
            vocab_size=self.vocab_size,
            device=self.device,
            use_attention=True,
            use_positional_encoding=False
        )
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        self.train_loader, self.val_loader = create_data_loaders(
            train_batch_size=batch_size,
            val_batch_size=batch_size,
            vocab_path=vocab_path
        )
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì •
        if self.use_class_weights:
            class_weights = self.train_loader.dataset.get_class_weights()
            self.criterion = nn.CrossEntropyLoss(weight=class_weights.to(self.device))
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-5
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=0.5,
            patience=10,
            verbose=True,
            min_lr=1e-7
        )
        
        # í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”
        self.start_epoch = 0
        self.best_accuracy = 0.0
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        
        # TensorBoard ì„¤ì •
        self.writer = SummaryWriter(log_dir=LOGS_PATH / "tensorboard")
        
        # ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ
        if resume_from is not None:
            self._load_checkpoint(resume_from)
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸš€ ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        self.logger.info(f"ğŸ”¢ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {count_parameters(self.model):,}")
        self.logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_loader.dataset):,}ê°œ")
        self.logger.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(self.val_loader.dataset):,}ê°œ")
        self.logger.info(f"ğŸ’¾ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ¯ ëª©í‘œ ì—í¬í¬: {num_epochs}")
        self.logger.info(f"ğŸ“ˆ í•™ìŠµë¥ : {learning_rate}")
        self.logger.info(f"âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {use_class_weights}")
        self.logger.info("=" * 60)
    
    def _load_checkpoint(self, checkpoint_path: Path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            checkpoint = load_checkpoint(checkpoint_path, self.model, self.optimizer)
            
            self.start_epoch = checkpoint['epoch'] + 1
            self.best_accuracy = checkpoint.get('accuracy', 0.0)
            
            # í•™ìŠµ ê¸°ë¡ ë³µì› (ìˆë‹¤ë©´)
            if 'train_losses' in checkpoint:
                self.train_losses = checkpoint['train_losses']
                self.val_losses = checkpoint['val_losses']
                self.train_accuracies = checkpoint['train_accuracies']
                self.val_accuracies = checkpoint['val_accuracies']
            
            self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ: ì—í¬í¬ {self.start_epoch}ë¶€í„° ì‹œì‘")
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.logger.info("ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    def _save_checkpoint(self, epoch: int, train_loss: float, val_loss: float, 
                        val_accuracy: float, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_data = {
            'epoch': epoch,
            'loss': val_loss,
            'accuracy': val_accuracy,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'vocab_size': self.vocab_size,
            'hyperparameters': {
                'learning_rate': self.learning_rate,
                'batch_size': self.batch_size,
                'gradient_clip_val': self.gradient_clip_val
            }
        }
        
        # ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = CHECKPOINTS_PATH / "last.pt"
        save_checkpoint(
            self.model.model,  # BiLSTMSignClassifier ì €ì¥
            self.optimizer,
            epoch,
            val_loss,
            val_accuracy,
            checkpoint_path,
            is_best=is_best
        )
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % self.save_every == 0:
            periodic_path = CHECKPOINTS_PATH / f"epoch_{epoch+1:03d}.pt"
            save_checkpoint(
                self.model.model,
                self.optimizer,
                epoch,
                val_loss,
                val_accuracy,
                periodic_path
            )
    
    def _compute_accuracy(self, outputs: torch.Tensor, labels: torch.Tensor) -> float:
        """ì •í™•ë„ ê³„ì‚°"""
        predictions = torch.argmax(outputs, dim=1)
        correct = (predictions == labels).sum().item()
        total = labels.size(0)
        return correct / total * 100.0
    
    def train_epoch(self, epoch: int) -> Tuple[float, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = len(self.train_loader)
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        pbar = tqdm(self.train_loader, desc=f"ì—í¬í¬ {epoch+1}/{self.num_epochs} [í•™ìŠµ]")
        
        for batch_idx, batch in enumerate(pbar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(self.device)
            
            # ìˆœì „íŒŒ
            self.optimizer.zero_grad()
            outputs = self.model(batch)
            
            loss = outputs['loss']
            logits = outputs['logits']
            labels = batch['label']
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê¸°ìš¸ê¸° í´ë¦¬í•‘
            if self.gradient_clip_val > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.gradient_clip_val
                )
            
            self.optimizer.step()
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            accuracy = self._compute_accuracy(logits, labels)
            
            total_loss += loss.item()
            total_accuracy += accuracy
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Acc': f"{accuracy:.2f}%",
                'LR': f"{self.optimizer.param_groups[0]['lr']:.6f}"
            })
            
            # TensorBoard ë¡œê¹… (ë°°ì¹˜ë³„)
            global_step = epoch * num_batches + batch_idx
            self.writer.add_scalar('Train/Loss_Step', loss.item(), global_step)
            self.writer.add_scalar('Train/Accuracy_Step', accuracy, global_step)
        
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        
        return avg_loss, avg_accuracy
    
    def validate_epoch(self, epoch: int) -> Tuple[float, float, Dict]:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = len(self.val_loader)
        
        # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f"ì—í¬í¬ {epoch+1}/{self.num_epochs} [ê²€ì¦]")
            
            for batch in pbar:
                # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(self.device)
                
                # ìˆœì „íŒŒ
                outputs = self.model(batch)
                
                loss = outputs['loss']
                logits = outputs['logits']
                labels = batch['label']
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                accuracy = self._compute_accuracy(logits, labels)
                
                total_loss += loss.item()
                total_accuracy += accuracy
                
                # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
                predictions = torch.argmax(logits, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
                pbar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Acc': f"{accuracy:.2f}%"
                })
        
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        
        # í´ë˜ìŠ¤ë³„ í†µê³„ ê³„ì‚°
        class_stats = self._compute_class_statistics(all_predictions, all_labels)
        
        return avg_loss, avg_accuracy, class_stats
    
    def _compute_class_statistics(self, predictions: List[int], labels: List[int]) -> Dict:
        """í´ë˜ìŠ¤ë³„ í†µê³„ ê³„ì‚°"""
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support
        
        # ì „ì²´ ì •í™•ë„
        overall_accuracy = accuracy_score(labels, predictions)
        
        # í´ë˜ìŠ¤ë³„ precision, recall, f1
        precision, recall, f1, support = precision_recall_fscore_support(
            labels, predictions, average=None, zero_division=0
        )
        
        # í‰ê·  ë©”íŠ¸ë¦­
        avg_precision = np.mean(precision)
        avg_recall = np.mean(recall)
        avg_f1 = np.mean(f1)
        
        return {
            'overall_accuracy': overall_accuracy * 100,
            'avg_precision': avg_precision * 100,
            'avg_recall': avg_recall * 100,
            'avg_f1': avg_f1 * 100,
            'class_precision': precision,
            'class_recall': recall,
            'class_f1': f1,
            'class_support': support
        }
    
    def train(self):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        self.logger.info("ğŸ¯ í•™ìŠµ ì‹œì‘!")
        start_time = time.time()
        
        try:
            for epoch in range(self.start_epoch, self.num_epochs):
                epoch_start_time = time.time()
                
                # í•™ìŠµ
                train_loss, train_accuracy = self.train_epoch(epoch)
                
                # ê²€ì¦
                val_loss, val_accuracy, class_stats = self.validate_epoch(epoch)
                
                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                self.scheduler.step(val_accuracy)
                
                # ë©”íŠ¸ë¦­ ì €ì¥
                self.train_losses.append(train_loss)
                self.val_losses.append(val_loss)
                self.train_accuracies.append(train_accuracy)
                self.val_accuracies.append(val_accuracy)
                
                # ìµœê³  ì„±ëŠ¥ ì²´í¬
                is_best = val_accuracy > self.best_accuracy
                if is_best:
                    self.best_accuracy = val_accuracy
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                self._save_checkpoint(epoch, train_loss, val_loss, val_accuracy, is_best)
                
                # TensorBoard ë¡œê¹… (ì—í¬í¬ë³„)
                self.writer.add_scalar('Train/Loss_Epoch', train_loss, epoch)
                self.writer.add_scalar('Train/Accuracy_Epoch', train_accuracy, epoch)
                self.writer.add_scalar('Val/Loss_Epoch', val_loss, epoch)
                self.writer.add_scalar('Val/Accuracy_Epoch', val_accuracy, epoch)
                self.writer.add_scalar('Val/Precision', class_stats['avg_precision'], epoch)
                self.writer.add_scalar('Val/Recall', class_stats['avg_recall'], epoch)
                self.writer.add_scalar('Val/F1', class_stats['avg_f1'], epoch)
                self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)
                
                # ì—í¬í¬ ê²°ê³¼ ë¡œê¹…
                epoch_time = time.time() - epoch_start_time
                
                self.logger.info("=" * 80)
                self.logger.info(f"ì—í¬í¬ {epoch+1}/{self.num_epochs} ì™„ë£Œ ({format_time(epoch_time)})")
                self.logger.info(f"ğŸ“ˆ í•™ìŠµ   - ì†ì‹¤: {train_loss:.4f}, ì •í™•ë„: {train_accuracy:.2f}%")
                self.logger.info(f"ğŸ“Š ê²€ì¦   - ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_accuracy:.2f}%")
                self.logger.info(f"ğŸ¯ ìƒì„¸   - P: {class_stats['avg_precision']:.2f}%, "
                               f"R: {class_stats['avg_recall']:.2f}%, F1: {class_stats['avg_f1']:.2f}%")
                self.logger.info(f"â­ ìµœê³    - {self.best_accuracy:.2f}% {'(NEW!)' if is_best else ''}")
                self.logger.info(f"ğŸ“š í•™ìŠµë¥  - {self.optimizer.param_groups[0]['lr']:.6f}")
                
                # ì¡°ê¸° ì¢…ë£Œ ì²´í¬ (ì„ íƒì )
                if self.optimizer.param_groups[0]['lr'] < 1e-7:
                    self.logger.info("í•™ìŠµë¥ ì´ ìµœì†Œê°’ì— ë„ë‹¬í•˜ì—¬ í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
        
        except KeyboardInterrupt:
            self.logger.info("ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            self.logger.error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        
        finally:
            # í•™ìŠµ ì™„ë£Œ ì²˜ë¦¬
            total_time = time.time() - start_time
            
            self.logger.info("=" * 80)
            self.logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
            self.logger.info("=" * 80)
            self.logger.info(f"â±ï¸ ì´ í•™ìŠµ ì‹œê°„: {format_time(total_time)}")
            self.logger.info(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {self.best_accuracy:.2f}%")
            self.logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {CHECKPOINTS_PATH}")
            self.logger.info("=" * 80)
            
            # í•™ìŠµ ê³¡ì„  ì €ì¥
            self._save_training_plots()
            
            # TensorBoard ì¢…ë£Œ
            self.writer.close()
    
    def _save_training_plots(self):
        """í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ì €ì¥"""
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            
            epochs = range(1, len(self.train_losses) + 1)
            
            # ì†ì‹¤ ê³¡ì„ 
            ax1.plot(epochs, self.train_losses, 'b-', label='í•™ìŠµ ì†ì‹¤', linewidth=2)
            ax1.plot(epochs, self.val_losses, 'r-', label='ê²€ì¦ ì†ì‹¤', linewidth=2)
            ax1.set_title('í•™ìŠµ/ê²€ì¦ ì†ì‹¤', fontsize=14, fontweight='bold')
            ax1.set_xlabel('ì—í¬í¬')
            ax1.set_ylabel('ì†ì‹¤')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ì •í™•ë„ ê³¡ì„ 
            ax2.plot(epochs, self.train_accuracies, 'b-', label='í•™ìŠµ ì •í™•ë„', linewidth=2)
            ax2.plot(epochs, self.val_accuracies, 'r-', label='ê²€ì¦ ì •í™•ë„', linewidth=2)
            ax2.set_title('í•™ìŠµ/ê²€ì¦ ì •í™•ë„', fontsize=14, fontweight='bold')
            ax2.set_xlabel('ì—í¬í¬')
            ax2.set_ylabel('ì •í™•ë„ (%)')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # ê³¼ì í•© ì§€í‘œ (í•™ìŠµ-ê²€ì¦ ì°¨ì´)
            train_val_gap = [t - v for t, v in zip(self.train_accuracies, self.val_accuracies)]
            ax3.plot(epochs, train_val_gap, 'g-', linewidth=2)
            ax3.set_title('ê³¼ì í•© ì§€í‘œ (í•™ìŠµ-ê²€ì¦ ì •í™•ë„ ì°¨ì´)', fontsize=14, fontweight='bold')
            ax3.set_xlabel('ì—í¬í¬')
            ax3.set_ylabel('ì •í™•ë„ ì°¨ì´ (%)')
            ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)
            ax3.grid(True, alpha=0.3)
            
            # ìµœê³  ì„±ëŠ¥ê¹Œì§€ì˜ ì§„í–‰
            best_epochs = []
            best_vals = []
            current_best = 0
            for i, val_acc in enumerate(self.val_accuracies):
                if val_acc > current_best:
                    current_best = val_acc
                    best_epochs.append(i + 1)
                    best_vals.append(val_acc)
            
            ax4.plot(best_epochs, best_vals, 'ro-', linewidth=2, markersize=6)
            ax4.set_title('ìµœê³  ì„±ëŠ¥ ì§„í–‰', fontsize=14, fontweight='bold')
            ax4.set_xlabel('ì—í¬í¬')
            ax4.set_ylabel('ìµœê³  ê²€ì¦ ì •í™•ë„ (%)')
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ì €ì¥
            plot_path = LOGS_PATH / "training_curves.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥: {plot_path}")
            
        except Exception as e:
            self.logger.warning(f"í•™ìŠµ ê³¡ì„  ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    trainer = SignLanguageTrainer(
        learning_rate=LEARNING_RATE,
        batch_size=BATCH_SIZE,
        num_epochs=NUM_EPOCHS,
        use_class_weights=True,
        gradient_clip_val=1.0,
        save_every=5
    )
    
    trainer.train()
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("ğŸ‰ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ í•™ìŠµì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#!/usr/bin/env python3
"""
SU:DA - ìˆ˜ì–´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
PyTorch Dataset êµ¬í˜„ìœ¼ë¡œ í•™ìŠµ/í‰ê°€ìš© ë°ì´í„° ë¡œë”©
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import random

from utils import (
    setup_logger, DATA_PROCESSED_PATH, pad_sequence,
    BATCH_SIZE
)
from vocab import SignVocabulary

class SignLanguageDataset(Dataset):
    """ìˆ˜ì–´ ì¸ì‹ìš© PyTorch Dataset"""
    
    def __init__(self, 
                 split: str = "train",
                 vocab_path: Optional[Path] = None,
                 max_sequence_length: int = 200,
                 augment: bool = False,
                 normalize: bool = True):
        """
        Args:
            split: ë°ì´í„° ë¶„í•  ("train" ë˜ëŠ” "val")
            vocab_path: ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            max_sequence_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            augment: ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€
            normalize: ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        """
        super().__init__()
        
        self.logger = setup_logger(f"SignDataset_{split}")
        self.split = split
        self.max_sequence_length = max_sequence_length
        self.augment = augment and (split == "train")  # í•™ìŠµì‹œì—ë§Œ ì¦ê°•
        self.normalize = normalize
        
        # ê²½ë¡œ ì„¤ì •
        self.processed_path = DATA_PROCESSED_PATH
        self.sequences_path = self.processed_path / "sequences"
        self.splits_path = self.processed_path / "splits"
        
        # ì‚¬ì „ ë¡œë”©
        self.vocab = SignVocabulary(vocab_path)
        self.logger.info(f"ì‚¬ì „ ë¡œë”© ì™„ë£Œ: {len(self.vocab)}ê°œ ë‹¨ì–´")
        
        # ë°ì´í„° ë©”íƒ€ë°ì´í„° ë¡œë”©
        self.metadata = self._load_metadata()
        self.logger.info(f"{split} ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {len(self.metadata)}ê°œ ìƒ˜í”Œ")
        
        # í†µê³„ ì •ë³´
        self._compute_statistics()
    
    def _load_metadata(self) -> pd.DataFrame:
        """ë¶„í• ëœ ë°ì´í„° ë©”íƒ€ë°ì´í„° ë¡œë”©"""
        metadata_file = self.splits_path / f"{self.split}.csv"
        
        if not metadata_file.exists():
            raise FileNotFoundError(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {metadata_file}")
        
        try:
            metadata = pd.read_csv(metadata_file)
            self.logger.info(f"ë©”íƒ€ë°ì´í„° ë¡œë”©: {len(metadata)}ê°œ ìƒ˜í”Œ")
            return metadata
            
        except Exception as e:
            raise RuntimeError(f"ë©”íƒ€ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _compute_statistics(self):
        """ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ê³„ì‚°"""
        self.num_samples = len(self.metadata)
        self.unique_words = self.metadata['word_label'].nunique()
        self.word_counts = self.metadata['word_label'].value_counts()
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„
        sequence_lengths = self.metadata['original_length'].values
        self.avg_sequence_length = np.mean(sequence_lengths)
        self.min_sequence_length = np.min(sequence_lengths)
        self.max_sequence_length_actual = np.max(sequence_lengths)
        
        self.logger.info(f"ğŸ“Š {self.split} ë°ì´í„°ì…‹ í†µê³„:")
        self.logger.info(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {self.num_samples}")
        self.logger.info(f"  - ê³ ìœ  ë‹¨ì–´ ìˆ˜: {self.unique_words}")
        self.logger.info(f"  - í‰ê·  ì‹œí€€ìŠ¤ ê¸¸ì´: {self.avg_sequence_length:.1f}")
        self.logger.info(f"  - ì‹œí€€ìŠ¤ ê¸¸ì´ ë²”ìœ„: {self.min_sequence_length}~{self.max_sequence_length_actual}")
    
    def _load_sequence(self, word_id: int) -> np.ndarray:
        """ì‹œí€€ìŠ¤ íŒŒì¼ ë¡œë”©"""
        sequence_file = self.sequences_path / f"WORD{word_id:04d}_sequence.npy"
        
        if not sequence_file.exists():
            raise FileNotFoundError(f"ì‹œí€€ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {sequence_file}")
        
        try:
            sequence = np.load(sequence_file)
            return sequence.astype(np.float32)
            
        except Exception as e:
            raise RuntimeError(f"ì‹œí€€ìŠ¤ ë¡œë”© ì‹¤íŒ¨ ({word_id}): {e}")
    
    def _normalize_sequence(self, sequence: np.ndarray) -> np.ndarray:
        """ì‹œí€€ìŠ¤ ì •ê·œí™”"""
        if not self.normalize:
            return sequence
        
        # í‚¤í¬ì¸íŠ¸ë³„ë¡œ ì •ê·œí™” (x, y, confidence êµ¬ì¡° ê³ ë ¤)
        normalized = sequence.copy()
        
        # ê° í”„ë ˆì„ë³„ë¡œ ì²˜ë¦¬
        for frame_idx in range(len(sequence)):
            frame = sequence[frame_idx]
            
            # 3ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬ (x, y, confidence)
            for i in range(0, len(frame), 3):
                if i + 2 < len(frame):
                    # x, y ì¢Œí‘œëŠ” 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                    normalized[frame_idx][i] = np.clip(frame[i], 0, 1)      # x
                    normalized[frame_idx][i+1] = np.clip(frame[i+1], 0, 1)  # y
                    # confidenceëŠ” 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                    normalized[frame_idx][i+2] = np.clip(frame[i+2], 0, 1)  # conf
        
        return normalized
    
    def _augment_sequence(self, sequence: np.ndarray) -> np.ndarray:
        """ë°ì´í„° ì¦ê°• (í•™ìŠµì‹œì—ë§Œ ì ìš©)"""
        if not self.augment:
            return sequence
        
        augmented = sequence.copy()
        
        # 1. ì‹œê°„ì¶• ë…¸ì´ì¦ˆ (í”„ë ˆì„ ìˆœì„œ ì•½ê°„ ë³€ê²½)
        if random.random() < 0.3 and len(sequence) > 5:
            # ì²˜ìŒê³¼ ë ëª‡ í”„ë ˆì„ì€ ê±´ë“œë¦¬ì§€ ì•Šê³  ì¤‘ê°„ë§Œ ì¡°ê¸ˆ ì„ê¸°
            start_idx = 2
            end_idx = len(sequence) - 2
            if end_idx > start_idx:
                # ì¤‘ê°„ êµ¬ê°„ì—ì„œ 1-2í”„ë ˆì„ ì •ë„ë§Œ ìœ„ì¹˜ ë°”ê¾¸ê¸°
                swap_idx1 = random.randint(start_idx, end_idx-1)
                swap_idx2 = min(swap_idx1 + 1, end_idx)
                augmented[swap_idx1], augmented[swap_idx2] = \
                    augmented[swap_idx2].copy(), augmented[swap_idx1].copy()
        
        # 2. í‚¤í¬ì¸íŠ¸ ë…¸ì´ì¦ˆ (ì‘ì€ ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€)
        if random.random() < 0.5:
            noise_scale = 0.02  # 2% ë…¸ì´ì¦ˆ
            noise = np.random.normal(0, noise_scale, augmented.shape)
            
            # confidence ê°’ì—ëŠ” ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            for i in range(2, augmented.shape[1], 3):  # confidence ì¸ë±ìŠ¤ë“¤
                noise[:, i] = 0
            
            augmented = augmented + noise
            
            # ë²”ìœ„ í´ë¦¬í•‘
            augmented = np.clip(augmented, 0, 1)
        
        # 3. ì‹œê°„ì¶• ìŠ¤ì¼€ì¼ë§ (ì†ë„ ë³€í™” ì‹œë®¬ë ˆì´ì…˜)
        if random.random() < 0.3:
            scale_factor = random.uniform(0.9, 1.1)  # Â±10% ì†ë„ ë³€í™”
            
            original_length = len(sequence)
            new_length = int(original_length * scale_factor)
            new_length = max(5, min(new_length, self.max_sequence_length))
            
            if new_length != original_length:
                # ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
                indices = np.linspace(0, original_length-1, new_length)
                resampled = np.zeros((new_length, augmented.shape[1]))
                
                for i, idx in enumerate(indices):
                    lower_idx = int(np.floor(idx))
                    upper_idx = min(int(np.ceil(idx)), original_length-1)
                    
                    if lower_idx == upper_idx:
                        resampled[i] = augmented[lower_idx]
                    else:
                        # ì„ í˜• ë³´ê°„
                        weight = idx - lower_idx
                        resampled[i] = (1 - weight) * augmented[lower_idx] + \
                                      weight * augmented[upper_idx]
                
                augmented = resampled
        
        return augmented
    
    def _pad_sequence(self, sequence: np.ndarray) -> np.ndarray:
        """ì‹œí€€ìŠ¤ íŒ¨ë”©"""
        return pad_sequence(sequence, self.max_sequence_length, pad_value=0.0)
    
    def __len__(self) -> int:
        """ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜"""
        return len(self.metadata)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """ë‹¨ì¼ ìƒ˜í”Œ ë°˜í™˜"""
        if idx >= len(self.metadata):
            raise IndexError(f"ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼: {idx} >= {len(self.metadata)}")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        sample_info = self.metadata.iloc[idx]
        word_id = sample_info['word_id']
        word_label = sample_info['word_label']
        original_length = sample_info['original_length']
        
        # ì‹œí€€ìŠ¤ ë¡œë”©
        try:
            sequence = self._load_sequence(word_id)
        except Exception as e:
            self.logger.error(f"ì‹œí€€ìŠ¤ ë¡œë”© ì‹¤íŒ¨ (idx={idx}, word_id={word_id}): {e}")
            # ë¹ˆ ì‹œí€€ìŠ¤ë¡œ ëŒ€ì²´
            sequence = np.zeros((self.max_sequence_length, 1629), dtype=np.float32)
            original_length = 0
        
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        sequence = self._normalize_sequence(sequence)
        sequence = self._augment_sequence(sequence)
        sequence = self._pad_sequence(sequence)
        
        # ë¼ë²¨ ì¸ë±ìŠ¤ ë³€í™˜
        label_idx = self.vocab.word_to_index(word_label)
        
        # í…ì„œ ë³€í™˜
        sequence_tensor = torch.from_numpy(sequence).float()
        label_tensor = torch.tensor(label_idx, dtype=torch.long)
        
        # ì¶”ê°€ ì •ë³´
        sample = {
            'sequence': sequence_tensor,        # [max_seq_len, keypoint_dim]
            'label': label_tensor,              # [1] - ë¼ë²¨ ì¸ë±ìŠ¤
            'word_id': torch.tensor(word_id, dtype=torch.long),
            'word_label': word_label,           # ì›ë³¸ ë‹¨ì–´ (ë¬¸ìì—´)
            'original_length': torch.tensor(original_length, dtype=torch.long),
            'sequence_mask': torch.tensor(
                [1.0 if i < original_length else 0.0 for i in range(self.max_sequence_length)], 
                dtype=torch.float
            )  # íŒ¨ë”© ë§ˆìŠ¤í¬
        }
        
        return sample
    
    def get_word_distribution(self) -> Dict[str, int]:
        """ë‹¨ì–´ë³„ ë¶„í¬ ë°˜í™˜"""
        return self.word_counts.to_dict()
    
    def get_sample_by_word(self, word_label: str, limit: int = 5) -> List[Dict]:
        """íŠ¹ì • ë‹¨ì–´ì˜ ìƒ˜í”Œë“¤ ë°˜í™˜"""
        word_samples = self.metadata[self.metadata['word_label'] == word_label]
        
        samples = []
        for idx in word_samples.index[:limit]:
            dataset_idx = self.metadata.index.get_loc(idx)
            sample = self[dataset_idx]
            samples.append(sample)
        
        return samples
    
    def get_class_weights(self) -> torch.Tensor:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        class_counts = np.zeros(len(self.vocab))
        
        for word_label in self.metadata['word_label']:
            label_idx = self.vocab.word_to_index(word_label)
            class_counts[label_idx] += 1
        
        # ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_samples = len(self.metadata)
        weights = total_samples / (len(self.vocab) * class_counts + 1e-8)  # zero division ë°©ì§€
        
        return torch.from_numpy(weights).float()
    
    def print_dataset_info(self):
        """ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥"""
        print("=" * 60)
        print(f"ğŸ“Š {self.split.upper()} ë°ì´í„°ì…‹ ì •ë³´")
        print("=" * 60)
        print(f"ğŸ”¢ ì´ ìƒ˜í”Œ ìˆ˜: {self.num_samples:,}")
        print(f"ğŸ“š ê³ ìœ  ë‹¨ì–´ ìˆ˜: {self.unique_words}")
        print(f"ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´: í‰ê·  {self.avg_sequence_length:.1f}, ë²”ìœ„ {self.min_sequence_length}~{self.max_sequence_length_actual}")
        print(f"ğŸ¯ ìµœëŒ€ íŒ¨ë”© ê¸¸ì´: {self.max_sequence_length}")
        print(f"ğŸ”„ ë°ì´í„° ì¦ê°•: {'ON' if self.augment else 'OFF'}")
        print(f"ğŸ“ ì •ê·œí™”: {'ON' if self.normalize else 'OFF'}")
        print()
        
        # ìƒìœ„ 10ê°œ ë‹¨ì–´ ë¶„í¬
        print("ğŸ† ìƒìœ„ 10ê°œ ë‹¨ì–´ ë¶„í¬:")
        top_words = self.word_counts.head(10)
        for word, count in top_words.items():
            percentage = (count / self.num_samples) * 100
            print(f"  {word}: {count}ê°œ ({percentage:.1f}%)")
        
        print("=" * 60)

def create_data_loaders(train_batch_size: int = BATCH_SIZE,
                       val_batch_size: int = BATCH_SIZE,
                       num_workers: int = 4,
                       pin_memory: bool = True,
                       vocab_path: Optional[Path] = None,
                       max_sequence_length: int = 200) -> Tuple[DataLoader, DataLoader]:
    """
    í•™ìŠµ/ê²€ì¦ìš© DataLoader ìƒì„±
    
    Args:
        train_batch_size: í•™ìŠµìš© ë°°ì¹˜ í¬ê¸°
        val_batch_size: ê²€ì¦ìš© ë°°ì¹˜ í¬ê¸°  
        num_workers: ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜
        pin_memory: GPU ë©”ëª¨ë¦¬ ê³ ì • ì‚¬ìš© ì—¬ë¶€
        vocab_path: ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
        max_sequence_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    
    Returns:
        (train_loader, val_loader)
    """
    logger = setup_logger("DataLoader")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SignLanguageDataset(
        split="train",
        vocab_path=vocab_path,
        max_sequence_length=max_sequence_length,
        augment=True,
        normalize=True
    )
    
    val_dataset = SignLanguageDataset(
        split="val", 
        vocab_path=vocab_path,
        max_sequence_length=max_sequence_length,
        augment=False,
        normalize=True
    )
    
    logger.info(f"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: í•™ìŠµ {len(train_dataset)}, ê²€ì¦ {len(val_dataset)}")
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=train_batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True,  # ë°°ì¹˜ í¬ê¸° ì¼ê´€ì„±ì„ ìœ„í•´
        persistent_workers=num_workers > 0
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=val_batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=False,
        persistent_workers=num_workers > 0
    )
    
    logger.info(f"DataLoader ìƒì„± ì™„ë£Œ: í•™ìŠµ {len(train_loader)} ë°°ì¹˜, ê²€ì¦ {len(val_loader)} ë°°ì¹˜")
    
    return train_loader, val_loader

def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """
    ë°°ì¹˜ ë°ì´í„° ê²°í•© í•¨ìˆ˜ (í•„ìš”ì‹œ ì‚¬ìš©)
    
    Args:
        batch: ìƒ˜í”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë°°ì¹˜ í…ì„œë“¤ì˜ ë”•ì…”ë„ˆë¦¬
    """
    # ê¸°ë³¸ì ìœ¼ë¡œëŠ” PyTorchì˜ default_collateê°€ ì˜ ì‘ë™í•˜ì§€ë§Œ,
    # ë³µì¡í•œ ë°°ì¹˜ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì„œ êµ¬í˜„
    
    sequences = torch.stack([item['sequence'] for item in batch])
    labels = torch.stack([item['label'] for item in batch])
    word_ids = torch.stack([item['word_id'] for item in batch])
    original_lengths = torch.stack([item['original_length'] for item in batch])
    sequence_masks = torch.stack([item['sequence_mask'] for item in batch])
    
    # ë¬¸ìì—´ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€
    word_labels = [item['word_label'] for item in batch]
    
    return {
        'sequence': sequences,
        'label': labels,
        'word_id': word_ids,
        'word_label': word_labels,
        'original_length': original_lengths,
        'sequence_mask': sequence_masks
    }

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§ª ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸")
    
    try:
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = SignLanguageDataset(split="train", augment=True)
        val_dataset = SignLanguageDataset(split="val", augment=False)
        
        # ì •ë³´ ì¶œë ¥
        train_dataset.print_dataset_info()
        val_dataset.print_dataset_info()
        
        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        if len(train_dataset) > 0:
            sample = train_dataset[0]
            print(f"\nğŸ“ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸:")
            print(f"  ì‹œí€€ìŠ¤ í¬ê¸°: {sample['sequence'].shape}")
            print(f"  ë¼ë²¨: {sample['label'].item()} -> '{sample['word_label']}'")
            print(f"  ì›ë³¸ ê¸¸ì´: {sample['original_length'].item()}")
            print(f"  ë‹¨ì–´ ID: {sample['word_id'].item()}")
        
        # DataLoader í…ŒìŠ¤íŠ¸
        train_loader, val_loader = create_data_loaders(
            train_batch_size=8, val_batch_size=8, num_workers=0
        )
        
        print(f"\nğŸ”„ DataLoader í…ŒìŠ¤íŠ¸:")
        batch = next(iter(train_loader))
        print(f"  ë°°ì¹˜ ì‹œí€€ìŠ¤ í¬ê¸°: {batch['sequence'].shape}")
        print(f"  ë°°ì¹˜ ë¼ë²¨ í¬ê¸°: {batch['label'].shape}")
        print(f"  ì²« ë²ˆì§¸ ìƒ˜í”Œ ë¼ë²¨: '{batch['word_label'][0]}'")
        
        print("\nâœ… ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()
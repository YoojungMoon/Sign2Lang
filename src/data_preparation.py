#!/usr/bin/env python3
"""
SU:DA - ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
raw ë°ì´í„°ë¥¼ processed í˜•íƒœë¡œ ë³€í™˜
- í”„ë ˆì„ë³„ í‚¤í¬ì¸íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ í•©ì¹˜ê¸°
- ì‹œê°„ ì •ë³´ë¥¼ í”„ë ˆì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
- train/val ë¶„í• 
- vocab ìƒì„±
"""

from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

# Stratified split ì‹œë„ â†’ ë¶ˆê°€ ì‹œ ëœë¤ ë¶„í• ë¡œ í´ë°±
from sklearn.model_selection import train_test_split
import random

from utils import (
    setup_logger, load_json, save_json, get_video_fps, time_to_frame_index,
    extract_keypoints_from_json, normalize_keypoints, pad_sequence,
    DATA_RAW_PATH, DATA_PROCESSED_PATH, WORD_START, WORD_END, SPEAKER_ID, DIRECTION,
    get_word_id_from_filename, is_valid_word_id, create_word_range
)


class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.logger = setup_logger("DataPreprocessor", "data_preparation.log")
        self.word_range = create_word_range()
        self.max_sequence_length = 200  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (í”„ë ˆì„ ìˆ˜)

        # ê²½ë¡œ ì„¤ì •
        self.raw_keypoints_path = DATA_RAW_PATH / "keypoints"
        self.raw_labels_path = DATA_RAW_PATH / "labels"
        self.raw_videos_path = DATA_RAW_PATH / "videos"

        self.processed_sequences_path = DATA_PROCESSED_PATH / "sequences"
        self.processed_splits_path = DATA_PROCESSED_PATH / "splits"
        self.processed_vocab_path = DATA_PROCESSED_PATH / "vocab"

        self.logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ: WORD{WORD_START}~{WORD_END} ({len(self.word_range)}ê°œ)")

    # ---------------------------
    # ë¡œë”©/ì „ì²˜ë¦¬ ìœ í‹¸
    # ---------------------------
    def load_morpheme_data(self, word_id: int) -> Optional[Dict]:
        """í˜•íƒœì†Œ ë°ì´í„° ë¡œë”©"""
        filename = f"NIA_SL_WORD{word_id:04d}_{SPEAKER_ID}_{DIRECTION}_morpheme.json"
        file_path = self.raw_labels_path / filename

        if not file_path.exists():
            self.logger.warning(f"í˜•íƒœì†Œ íŒŒì¼ ì—†ìŒ: {filename}")
            return None

        try:
            return load_json(file_path)
        except Exception as e:
            self.logger.error(f"í˜•íƒœì†Œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ ({filename}): {e}")
            return None

    def load_keypoint_sequence(self, word_id: int) -> Optional[np.ndarray]:
        """í”„ë ˆì„ë³„ í‚¤í¬ì¸íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ í•©ì¹˜ê¸°"""
        folder_name = f"NIA_SL_WORD{word_id:04d}_{SPEAKER_ID}_{DIRECTION}"
        keypoints_folder = self.raw_keypoints_path / folder_name

        if not keypoints_folder.exists():
            self.logger.warning(f"í‚¤í¬ì¸íŠ¸ í´ë” ì—†ìŒ: {folder_name}")
            return None

        # í”„ë ˆì„ë³„ íŒŒì¼ë“¤ ìˆ˜ì§‘
        frame_files = []
        for file_path in keypoints_folder.glob("*.json"):
            if "_keypoints.json" in file_path.name:
                # íŒŒì¼ëª…ì—ì„œ í”„ë ˆì„ ë²ˆí˜¸ ì¶”ì¶œ (â€¦_000000000000_keypoints.json í˜•íƒœ)
                try:
                    frame_num_str = file_path.stem.split('_')[-2]
                    frame_num = int(frame_num_str)
                    frame_files.append((frame_num, file_path))
                except (ValueError, IndexError):
                    continue

        if not frame_files:
            self.logger.warning(f"í‚¤í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {folder_name}")
            return None

        # í”„ë ˆì„ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
        frame_files.sort(key=lambda x: x[0])

        # ê° í”„ë ˆì„ì˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        sequence_keypoints = []
        for frame_num, file_path in frame_files:
            try:
                keypoint_json = load_json(file_path)
                keypoints = extract_keypoints_from_json(keypoint_json)
                normalized_keypoints = normalize_keypoints(keypoints)
                sequence_keypoints.append(normalized_keypoints)
            except Exception as e:
                self.logger.warning(f"í”„ë ˆì„ {frame_num} í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                # ë¹ˆ í‚¤í¬ì¸íŠ¸ë¡œ ëŒ€ì²´ (543 landmarks * 3)
                sequence_keypoints.append(np.zeros(543 * 3, dtype=np.float32))

        if not sequence_keypoints:
            return None

        sequence = np.array(sequence_keypoints, dtype=np.float32)
        self.logger.debug(f"í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ìƒì„±: {folder_name}, shape={sequence.shape}")
        return sequence

    def get_video_metadata(self, word_id: int) -> Optional[Dict]:
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (í˜„ì¬ëŠ” FPSë§Œ ì‚¬ìš©)"""
        filename = f"NIA_SL_WORD{word_id:04d}_{SPEAKER_ID}_{DIRECTION}.mp4"
        video_path = self.raw_videos_path / filename

        if not video_path.exists():
            self.logger.warning(f"ë¹„ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {filename}")
            return None

        try:
            fps = get_video_fps(video_path)
            return {"fps": fps, "path": str(video_path)}
        except Exception as e:
            self.logger.error(f"ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({filename}): {e}")
            return None

    # ---------------------------
    # ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬
    # ---------------------------
    def process_single_word(self, word_id: int) -> Optional[Dict]:
        """ë‹¨ì¼ ë‹¨ì–´ ë°ì´í„° ì²˜ë¦¬"""
        # 1) í˜•íƒœì†Œ
        morpheme_data = self.load_morpheme_data(word_id)
        if morpheme_data is None:
            return None

        # 2) í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤
        keypoint_sequence = self.load_keypoint_sequence(word_id)
        if keypoint_sequence is None:
            return None

        # 3) ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„°
        video_metadata = self.get_video_metadata(word_id)
        if video_metadata is None:
            return None

        # 4) ë¼ë²¨/ì‹œê°„ ì •ë³´
        try:
            data_entries = morpheme_data.get("data", [])
            if not data_entries:
                self.logger.warning(f"í˜•íƒœì†Œ ë°ì´í„° ì—†ìŒ: WORD{word_id}")
                return None

            first_entry = data_entries[0]
            start_time = float(first_entry.get("start", 0))
            end_time = float(first_entry.get("end", 0))

            attributes = first_entry.get("attributes", [])
            if not attributes:
                self.logger.warning(f"ìˆ˜ì–´ ë‹¨ì–´ ë¼ë²¨ ì—†ìŒ: WORD{word_id}")
                return None

            word_label = attributes[0].get("name", "")
            if not word_label:
                self.logger.warning(f"ìˆ˜ì–´ ë‹¨ì–´ëª… ì—†ìŒ: WORD{word_id}")
                return None
        except (KeyError, ValueError, IndexError) as e:
            self.logger.error(f"í˜•íƒœì†Œ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ WORD{word_id}: {e}")
            return None

        # 5) ì‹œê°„ì„ í”„ë ˆì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        fps = video_metadata["fps"]
        start_frame = time_to_frame_index(start_time, fps)
        end_frame = time_to_frame_index(end_time, fps)

        # í”„ë ˆì„ ë²”ìœ„ ê²€ì¦
        seq_len = len(keypoint_sequence)
        start_frame = max(0, min(start_frame, seq_len - 1))
        end_frame = max(start_frame + 1, min(end_frame, seq_len))

        # 6) ì‹œí€€ìŠ¤ íŒ¨ë”© (ëª¨ë¸ ì…ë ¥ ê¸¸ì´ í†µì¼)
        padded_sequence = pad_sequence(keypoint_sequence, self.max_sequence_length)

        # 7) ê²°ê³¼ ë°˜í™˜ (ì‹œí€€ìŠ¤ëŠ” ë³„ë„ íŒŒì¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ ë©”íƒ€ì™€ í•¨ê»˜ ë°˜í™˜)
        return {
            "word_id": word_id,
            "word_label": word_label,
            "sequence": padded_sequence,
            "original_length": seq_len,
            "start_frame": start_frame,
            "end_frame": end_frame,
            "start_time": start_time,
            "end_time": end_time,
            "fps": fps,
            "video_duration": morpheme_data.get("metaData", {}).get("duration", 0),
        }

    def save_processed_sequence(self, word_id: int, sequence: np.ndarray):
        """ì²˜ë¦¬ëœ ì‹œí€€ìŠ¤ë¥¼ .npy íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"WORD{word_id:04d}_sequence.npy"
        filepath = self.processed_sequences_path / filename
        self.processed_sequences_path.mkdir(parents=True, exist_ok=True)
        np.save(filepath, sequence)

    # ---------------------------
    # ë‹¤ìˆ˜ ìƒ˜í”Œ ì²˜ë¦¬/ì–´íœ˜ì§‘/ë¶„í• 
    # ---------------------------
    def process_all_words(self) -> List[Dict]:
        """ëª¨ë“  ë‹¨ì–´ ë°ì´í„° ì²˜ë¦¬"""
        self.logger.info("ì „ì²´ ë‹¨ì–´ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")

        processed_samples: List[Dict] = []
        failed_words: List[int] = []

        for word_id in tqdm(self.word_range, desc="ë°ì´í„° ì²˜ë¦¬ ì¤‘"):
            if not is_valid_word_id(word_id):
                continue

            processed_data = self.process_single_word(word_id)
            if processed_data is not None:
                # ì‹œí€€ìŠ¤ëŠ” íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ë©”íƒ€ë°ì´í„°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ìœ ì§€
                self.save_processed_sequence(word_id, processed_data["sequence"])
                metadata = {k: v for k, v in processed_data.items() if k != "sequence"}
                processed_samples.append(metadata)
            else:
                failed_words.append(word_id)

        self.logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {len(processed_samples)}ê°œ, ì‹¤íŒ¨ {len(failed_words)}ê°œ")
        if failed_words:
            self.logger.warning(f"ì²˜ë¦¬ ì‹¤íŒ¨í•œ ë‹¨ì–´ë“¤: {failed_words[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ

        return processed_samples

    def create_vocabulary(self, processed_samples: List[Dict]) -> Dict[str, int]:
        """ìˆ˜ì–´ ë‹¨ì–´ ì‚¬ì „ ìƒì„± ë° ì €ì¥ (json/txt)"""
        self.logger.info("ìˆ˜ì–´ ë‹¨ì–´ ì‚¬ì „ ìƒì„± ì¤‘")

        unique_words = {s["word_label"] for s in processed_samples}

        # íŠ¹ìˆ˜ í† í°
        vocab: Dict[str, int] = {
            "<PAD>": 0,
            "<UNK>": 1,
            "<SOS>": 2,
            "<EOS>": 3,
        }

        # ë‹¨ì–´ ì‚¬ì „ì€ ì •ë ¬ í›„ 4ë¶€í„° ë¶€ì—¬
        for i, word in enumerate(sorted(unique_words)):
            vocab[word] = i + 4

        self.logger.info(f"ìˆ˜ì–´ ë‹¨ì–´ ì‚¬ì „ ìƒì„± ì™„ë£Œ: {len(vocab)}ê°œ ë‹¨ì–´ (íŠ¹ìˆ˜í† í° 4ê°œ í¬í•¨)")

        self.processed_vocab_path.mkdir(parents=True, exist_ok=True)
        save_json(vocab, self.processed_vocab_path / "vocab.json")

        # ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ txt ë²„ì „ë„ ì €ì¥
        with open(self.processed_vocab_path / "vocab.txt", "w", encoding="utf-8") as f:
            for word, idx in sorted(vocab.items(), key=lambda x: x[1]):
                f.write(f"{idx}\t{word}\n")

        return vocab

    def split_train_val(
        self,
        processed_samples: List[Dict],
        test_size: float = 0.2,
        random_state: int = 42,
    ) -> Tuple[List[Dict], List[Dict]]:
        """í•™ìŠµìš©/ê²€ì¦ìš© ë°ì´í„° ë¶„í•  (ê°€ëŠ¥í•˜ë©´ stratify, ì•„ë‹ˆë©´ ëœë¤)"""
        self.logger.info(f"ë°ì´í„° ë¶„í•  ì¤‘ (ê²€ì¦ìš© ë¹„ìœ¨: {test_size})")

        labels = [s["word_label"] for s in processed_samples]

        # ìš°ì„  stratified splitì„ ì‹œë„
        try:
            train_samples, val_samples = train_test_split(
                processed_samples,
                test_size=test_size,
                random_state=random_state,
                stratify=labels,
                shuffle=True,
            )
        except ValueError:
            # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ê°€ 1ê°œì¸ ê²½ìš° ë“± â†’ ëœë¤ ë¶„í• ë¡œ í´ë°±
            from collections import Counter

            label_counts = Counter(labels)
            single_sample_classes = [lab for lab, cnt in label_counts.items() if cnt == 1]
            if single_sample_classes:
                self.logger.warning(f"1ê°œ ìƒ˜í”Œë§Œ ìˆëŠ” í´ë˜ìŠ¤ë“¤: {len(single_sample_classes)}ê°œ")
                self.logger.warning(f"ì˜ˆì‹œ: {single_sample_classes[:5]}")

            random.seed(random_state)
            shuffled = processed_samples.copy()
            random.shuffle(shuffled)

            split_idx = int(len(shuffled) * (1 - test_size))
            train_samples = shuffled[:split_idx]
            val_samples = shuffled[split_idx:]

        self.logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: í•™ìŠµìš© {len(train_samples)}ê°œ, ê²€ì¦ìš© {len(val_samples)}ê°œ")
        return train_samples, val_samples

    def save_splits(self, train_samples: List[Dict], val_samples: List[Dict]) -> Dict:
        """ë¶„í• ëœ ë°ì´í„° CSV + ìš”ì•½ ì €ì¥"""
        self.processed_splits_path.mkdir(parents=True, exist_ok=True)

        # CSV ì €ì¥
        train_df = pd.DataFrame(train_samples)
        val_df = pd.DataFrame(val_samples)
        train_file = self.processed_splits_path / "train.csv"
        val_file = self.processed_splits_path / "val.csv"
        train_df.to_csv(train_file, index=False, encoding="utf-8")
        val_df.to_csv(val_file, index=False, encoding="utf-8")
        self.logger.info(f"ë¶„í•  ë°ì´í„° ì €ì¥ ì™„ë£Œ: {train_file}, {val_file}")

        # ìš”ì•½ ì €ì¥
        total = len(train_samples) + len(val_samples)
        summary = {
            "total_samples": total,
            "train_samples": len(train_samples),
            "val_samples": len(val_samples),
            "train_ratio": len(train_samples) / max(total, 1),
            "val_ratio": len(val_samples) / max(total, 1),
            "unique_words": len(set(s["word_label"] for s in train_samples + val_samples)),
            "max_sequence_length": self.max_sequence_length,
            "word_range": f"{WORD_START}-{WORD_END}",
            "speaker": SPEAKER_ID,
            "direction": DIRECTION,
        }
        save_json(summary, self.processed_splits_path / "summary.json")
        return summary

    # ---------------------------
    # íŒŒì´í”„ë¼ì¸
    # ---------------------------
    def run_preprocessing(self) -> bool:
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸš€ SU:DA ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        self.logger.info("=" * 60)

        try:
            # 1) ì „ì²´ ë‹¨ì–´ ì²˜ë¦¬
            processed_samples = self.process_all_words()
            if not processed_samples:
                self.logger.error("ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return False

            # 2) ë‹¨ì–´ ì‚¬ì „
            _ = self.create_vocabulary(processed_samples)

            # 3) í•™ìŠµ/ê²€ì¦ ë¶„í• 
            train_samples, val_samples = self.split_train_val(processed_samples)

            # 4) ë¶„í•  ì €ì¥ + ìš”ì•½
            summary = self.save_splits(train_samples, val_samples)

            # 5) ë¡œê·¸ ìš”ì•½
            self.logger.info("=" * 60)
            self.logger.info("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {summary['total_samples']}")
            self.logger.info(f"ğŸ¯ í•™ìŠµìš©: {summary['train_samples']}ê°œ ({summary['train_ratio']:.1%})")
            self.logger.info(f"ğŸ” ê²€ì¦ìš©: {summary['val_samples']}ê°œ ({summary['val_ratio']:.1%})")
            self.logger.info(f"ğŸ“š ìˆ˜ì–´ ë‹¨ì–´ ìˆ˜: {summary['unique_words']}ê°œ")
            self.logger.info(f"ğŸ“ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {summary['max_sequence_length']}")
            self.logger.info(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {DATA_PROCESSED_PATH}")
            self.logger.info("=" * 60)
            return True

        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë‹¨ë… ì‹¤í–‰ ì‹œ í¸ì˜ìš©"""
    preprocessor = DataPreprocessor()
    ok = preprocessor.run_preprocessing()
    if ok:
        print("ğŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
